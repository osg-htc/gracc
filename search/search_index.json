{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GRACC GRid ACcounting Collector (pronounced \"grok\") GRACC is a collection of components for implementing resource usage accounting. Why GRACC? GRACC is meant as a replacement of the Gratia accounting system; the engineering focus is on smaller, independent components rather than a monolithic collector architecture. The hope is that, by breaking the functionality into a series of smaller components, future architectural changes (such as migration to a new database) can be done without rewriting the entire infrastructure. For example, forwarding information to a separate accounting database becomes much simpler in this infrastructure. Repositories of interest: GRACC Collector . An agent which runs on an existing Gratia collector that forwards raw usage records to GRACC. GRACC Monitoring Emails . Simple daily emails overview GRACC activity. GRACC Request Agent . Agents that listen for replay requests from GRACC. GRACC Summary Agent . Agents that request summary records from the Request Agent and forward the records back to the Logstash agent for storage in ElasticSearch GRACC Archiver . Agents that listen on the RabbitMQ exchange for raw records and store them to disk in a gzip file. GRACC Backup Scripts . A service that runs periodically on the same host as the Archiver which backs up the completed gzip files to external Tape. See Backup Docs for more details. GRACC Reports . A collection of reports that run periodically that query GRACC and present aggregated data to stakeholders.","title":"Introduction"},{"location":"#gracc","text":"GRid ACcounting Collector (pronounced \"grok\") GRACC is a collection of components for implementing resource usage accounting.","title":"GRACC"},{"location":"#why-gracc","text":"GRACC is meant as a replacement of the Gratia accounting system; the engineering focus is on smaller, independent components rather than a monolithic collector architecture. The hope is that, by breaking the functionality into a series of smaller components, future architectural changes (such as migration to a new database) can be done without rewriting the entire infrastructure. For example, forwarding information to a separate accounting database becomes much simpler in this infrastructure. Repositories of interest: GRACC Collector . An agent which runs on an existing Gratia collector that forwards raw usage records to GRACC. GRACC Monitoring Emails . Simple daily emails overview GRACC activity. GRACC Request Agent . Agents that listen for replay requests from GRACC. GRACC Summary Agent . Agents that request summary records from the Request Agent and forward the records back to the Logstash agent for storage in ElasticSearch GRACC Archiver . Agents that listen on the RabbitMQ exchange for raw records and store them to disk in a gzip file. GRACC Backup Scripts . A service that runs periodically on the same host as the Archiver which backs up the completed gzip files to external Tape. See Backup Docs for more details. GRACC Reports . A collection of reports that run periodically that query GRACC and present aggregated data to stakeholders.","title":"Why GRACC?"},{"location":"content/corrections/","text":"Corrections GRACC corrections are maintained in an Elasticsearch index ( gracc.corrections-0 as of this writing). These are used by gracc-request during summarization to correct/normalize VO and Project names. The gracc-correct tool provides a simple interface to manage name corrections. It is a single Python script that requires the elasticseach package ( pip install elasticsearch ). Currently GRACC does not expose a read-write interface to Elasticsearch, so the script must be run from a node within the GRACC cluster (typically the head/client node). Note that it might take a minute for any changes to be reflected in queries to Elasticsearch. Usage gracc-correct [-h] [--url URL] [--index INDEX] {project,vo} {list,add,update,delete} ... positional arguments: {project,vo} Correction type {list,add,update,delete} list print existing corrections add create new correction update update existing correction delete delete existing correction(s) optional arguments: -h, --help show this help message and exit --url URL Elasticsearch URL --index INDEX Index containing corrections list command The list command will fetch and print the corrections in JSON format. You can limit the number of results with the --size option (default 1000), or filter the results by providing a JSON document with fields to match, or an arbitrary Elasticsearch query. Extra options: --size SIZE Max number of documents to list. --query QUERY Search query to limit results. --doc DOC Optional JSON document with correction source to match add command The add command with create a new correction. You can provide a JSON document, or gracc-correct will prompt you for the fields. If a correction already exists with the given keys, you'll be given the option to update the correction. Extra options: --doc DOC Optional JSON document with correction source to match update command The update command will update an existing correction. You can provide a JSON document, or gracc-correct will prompt you for the fields. If a correction doesn't already exist with the given keys, you'll be given the option to create a new correction. Extra options: --doc DOC Optional JSON document with correction source to match delete command The delete command will delete an existing correction. You can provide a JSON document, a correction ID, or gracc-correct will prompt you for the fields. It will display the matching correction, and prompt for confirmation. This will be repeated if multiple documents match. Extra options: --doc DOC Optional JSON document with correction source to delete. --id ID Document id to delete. Examples List VO corrections $ gracc-correct vo list --size 5 18 {\"ReportableVOName\": \"nanohub\", \"VOName\": \"/nanohub/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"nanohub\"} 19 {\"ReportableVOName\": \"ops\", \"VOName\": \"/ops/Role=lcgadmin/Capability=NULL\", \"CorrectedVOName\": \"ops\"} 20 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"osg\"} 21 {\"ReportableVOName\": \"osgedu\", \"VOName\": \"/osgedu/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"osgedu\"} 22 {\"ReportableVOName\": \"star\", \"VOName\": \"/star/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"star\"} List Project corrections with query $ gracc-correct project list --query osg 7 {\"CorrectedProjectName\": \"OSG-Staff\", \"ProjectName\": \"OSG-Staff\"} 24 {\"CorrectedProjectName\": \"OSG-STAFF\", \"ProjectName\": \"OSG-STAFF\"} 200 {\"CorrectedProjectName\": \"SWC-OSG-UC14\", \"ProjectName\": \"SWC-OSG-UC14\"} 209 {\"CorrectedProjectName\": \"SWC-OSG-IU15\", \"ProjectName\": \"SWC-OSG-IU15\"} 104 {\"CorrectedProjectName\": \"osg\", \"ProjectName\": \"osg\"} 448 {\"CorrectedProjectName\": \"OSG Staff\", \"ProjectName\": \"OSG Staff\"} 51 {\"CorrectedProjectName\": \"ConnectTrain\", \"ProjectName\": \"OSG-Connect-test\"} 52 {\"CorrectedProjectName\": \"ConnectTrain\", \"ProjectName\": \"OSG-Connect\"} 43 {\"CorrectedProjectName\": \"SNOplus\", \"ProjectName\": \"OSG-PHY00101\"} List VO corrections matching document $ gracc-correct vo list --doc '{\"ReportableVOName\":\"osg\"}' --size 5 20 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"osg\"} 495 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/Role=pilot/Capability=NULL\", \"CorrectedVOName\": \"osg\"} 920 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/LocalGroup=external\", \"CorrectedVOName\": \"osg\"} 1041 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/LocalGroup=marksant\", \"CorrectedVOName\": \"osg\"} 1042 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/Snowmass/Role=snowmassadmin/Capability=NULL\", \"CorrectedVOName\": \"osg\"} Add new VO correction interactively $ gracc-correct vo add Field(s) to correct: VOName: example ReportableVOName: example Corrected VOName: osg Correction created. id: AVyOq_xvTIq8btIx9sGM $ gracc-correct vo list --query '_id:AVyOq_xvTIq8btIx9sGM' AVyOq_xvTIq8btIx9sGM {\"VOName\": \"example\", \"ReportableVOName\": \"example\", \"CorrectedVOName\": \"osg\"} Add new VO correction with doc $ gracc-correct vo add --doc '{\"VOName\":\"example2\",\"ReportableVOName\":\"example\",\"CorrectedVOName\":\"osg\"}' Correction created. id: AVyOr3nYTIq8btIx9sGN $ gracc-correct vo list --query '_id:AVyOr3nYTIq8btIx9sGN' AVyOr3nYTIq8btIx9sGN {\"VOName\": \"example2\", \"ReportableVOName\": \"example\", \"CorrectedVOName\": \"osg\"} Update VO correction interactively $ gracc-correct vo update Field(s) to correct: VOName: example2 ReportableVOName: example Corrected VOName: fermilab Correction AVyOr3nYTIq8btIx9sGN updated. $ gracc-correct vo list --query '_id:AVyOr3nYTIq8btIx9sGN' AVyOr3nYTIq8btIx9sGN {\"VOName\": \"example2\", \"ReportableVOName\": \"example\", \"CorrectedVOName\": \"fermilab\"} Note: it might take a minute for the update to be reflected in the query. Delete correction interactively $ gracc-correct vo delete Field(s) to correct: VOName: example ReportableVOName: example Corrected VOName: osg {u'VOName': u'example', u'ReportableVOName': u'example', u'CorrectedVOName': u'osg'} Delete record? (Y/N) y Correction AVyOq_xvTIq8btIx9sGM deleted. Delete correction with doc $ gracc-correct vo delete --doc '{\"VOName\": \"example2\", \"ReportableVOName\": \"example\", \"CorrectedVOName\": \"fermilab\"}' {u'VOName': u'example2', u'ReportableVOName': u'example', u'CorrectedVOName': u'fermilab'} Delete record? (Y/N) y Correction AVyOr3nYTIq8btIx9sGN deleted. Delete correction with id $ gracc-correct vo delete --id AVyOwmxFTIq8btIx9sGP {u'VOName': u'example', u'ReportableVOName': u'example', u'CorrectedVOName': u'osg'} Delete record? (Y/N) y Correction AVyOwmxFTIq8btIx9sGP deleted.","title":"Corrections"},{"location":"content/corrections/#corrections","text":"GRACC corrections are maintained in an Elasticsearch index ( gracc.corrections-0 as of this writing). These are used by gracc-request during summarization to correct/normalize VO and Project names. The gracc-correct tool provides a simple interface to manage name corrections. It is a single Python script that requires the elasticseach package ( pip install elasticsearch ). Currently GRACC does not expose a read-write interface to Elasticsearch, so the script must be run from a node within the GRACC cluster (typically the head/client node). Note that it might take a minute for any changes to be reflected in queries to Elasticsearch.","title":"Corrections"},{"location":"content/corrections/#usage","text":"gracc-correct [-h] [--url URL] [--index INDEX] {project,vo} {list,add,update,delete} ... positional arguments: {project,vo} Correction type {list,add,update,delete} list print existing corrections add create new correction update update existing correction delete delete existing correction(s) optional arguments: -h, --help show this help message and exit --url URL Elasticsearch URL --index INDEX Index containing corrections","title":"Usage"},{"location":"content/corrections/#list-command","text":"The list command will fetch and print the corrections in JSON format. You can limit the number of results with the --size option (default 1000), or filter the results by providing a JSON document with fields to match, or an arbitrary Elasticsearch query.","title":"list command"},{"location":"content/corrections/#extra-options","text":"--size SIZE Max number of documents to list. --query QUERY Search query to limit results. --doc DOC Optional JSON document with correction source to match","title":"Extra options:"},{"location":"content/corrections/#add-command","text":"The add command with create a new correction. You can provide a JSON document, or gracc-correct will prompt you for the fields. If a correction already exists with the given keys, you'll be given the option to update the correction.","title":"add command"},{"location":"content/corrections/#extra-options_1","text":"--doc DOC Optional JSON document with correction source to match","title":"Extra options:"},{"location":"content/corrections/#update-command","text":"The update command will update an existing correction. You can provide a JSON document, or gracc-correct will prompt you for the fields. If a correction doesn't already exist with the given keys, you'll be given the option to create a new correction.","title":"update command"},{"location":"content/corrections/#extra-options_2","text":"--doc DOC Optional JSON document with correction source to match","title":"Extra options:"},{"location":"content/corrections/#delete-command","text":"The delete command will delete an existing correction. You can provide a JSON document, a correction ID, or gracc-correct will prompt you for the fields. It will display the matching correction, and prompt for confirmation. This will be repeated if multiple documents match.","title":"delete command"},{"location":"content/corrections/#extra-options_3","text":"--doc DOC Optional JSON document with correction source to delete. --id ID Document id to delete.","title":"Extra options:"},{"location":"content/corrections/#examples","text":"","title":"Examples"},{"location":"content/corrections/#list-vo-corrections","text":"$ gracc-correct vo list --size 5 18 {\"ReportableVOName\": \"nanohub\", \"VOName\": \"/nanohub/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"nanohub\"} 19 {\"ReportableVOName\": \"ops\", \"VOName\": \"/ops/Role=lcgadmin/Capability=NULL\", \"CorrectedVOName\": \"ops\"} 20 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"osg\"} 21 {\"ReportableVOName\": \"osgedu\", \"VOName\": \"/osgedu/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"osgedu\"} 22 {\"ReportableVOName\": \"star\", \"VOName\": \"/star/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"star\"}","title":"List VO corrections"},{"location":"content/corrections/#list-project-corrections-with-query","text":"$ gracc-correct project list --query osg 7 {\"CorrectedProjectName\": \"OSG-Staff\", \"ProjectName\": \"OSG-Staff\"} 24 {\"CorrectedProjectName\": \"OSG-STAFF\", \"ProjectName\": \"OSG-STAFF\"} 200 {\"CorrectedProjectName\": \"SWC-OSG-UC14\", \"ProjectName\": \"SWC-OSG-UC14\"} 209 {\"CorrectedProjectName\": \"SWC-OSG-IU15\", \"ProjectName\": \"SWC-OSG-IU15\"} 104 {\"CorrectedProjectName\": \"osg\", \"ProjectName\": \"osg\"} 448 {\"CorrectedProjectName\": \"OSG Staff\", \"ProjectName\": \"OSG Staff\"} 51 {\"CorrectedProjectName\": \"ConnectTrain\", \"ProjectName\": \"OSG-Connect-test\"} 52 {\"CorrectedProjectName\": \"ConnectTrain\", \"ProjectName\": \"OSG-Connect\"} 43 {\"CorrectedProjectName\": \"SNOplus\", \"ProjectName\": \"OSG-PHY00101\"}","title":"List Project corrections with query"},{"location":"content/corrections/#list-vo-corrections-matching-document","text":"$ gracc-correct vo list --doc '{\"ReportableVOName\":\"osg\"}' --size 5 20 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/Role=NULL/Capability=NULL\", \"CorrectedVOName\": \"osg\"} 495 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/Role=pilot/Capability=NULL\", \"CorrectedVOName\": \"osg\"} 920 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/LocalGroup=external\", \"CorrectedVOName\": \"osg\"} 1041 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/LocalGroup=marksant\", \"CorrectedVOName\": \"osg\"} 1042 {\"ReportableVOName\": \"osg\", \"VOName\": \"/osg/Snowmass/Role=snowmassadmin/Capability=NULL\", \"CorrectedVOName\": \"osg\"}","title":"List VO corrections matching document"},{"location":"content/corrections/#add-new-vo-correction-interactively","text":"$ gracc-correct vo add Field(s) to correct: VOName: example ReportableVOName: example Corrected VOName: osg Correction created. id: AVyOq_xvTIq8btIx9sGM $ gracc-correct vo list --query '_id:AVyOq_xvTIq8btIx9sGM' AVyOq_xvTIq8btIx9sGM {\"VOName\": \"example\", \"ReportableVOName\": \"example\", \"CorrectedVOName\": \"osg\"}","title":"Add new VO correction interactively"},{"location":"content/corrections/#add-new-vo-correction-with-doc","text":"$ gracc-correct vo add --doc '{\"VOName\":\"example2\",\"ReportableVOName\":\"example\",\"CorrectedVOName\":\"osg\"}' Correction created. id: AVyOr3nYTIq8btIx9sGN $ gracc-correct vo list --query '_id:AVyOr3nYTIq8btIx9sGN' AVyOr3nYTIq8btIx9sGN {\"VOName\": \"example2\", \"ReportableVOName\": \"example\", \"CorrectedVOName\": \"osg\"}","title":"Add new VO correction with doc"},{"location":"content/corrections/#update-vo-correction-interactively","text":"$ gracc-correct vo update Field(s) to correct: VOName: example2 ReportableVOName: example Corrected VOName: fermilab Correction AVyOr3nYTIq8btIx9sGN updated. $ gracc-correct vo list --query '_id:AVyOr3nYTIq8btIx9sGN' AVyOr3nYTIq8btIx9sGN {\"VOName\": \"example2\", \"ReportableVOName\": \"example\", \"CorrectedVOName\": \"fermilab\"} Note: it might take a minute for the update to be reflected in the query.","title":"Update VO correction interactively"},{"location":"content/corrections/#delete-correction-interactively","text":"$ gracc-correct vo delete Field(s) to correct: VOName: example ReportableVOName: example Corrected VOName: osg {u'VOName': u'example', u'ReportableVOName': u'example', u'CorrectedVOName': u'osg'} Delete record? (Y/N) y Correction AVyOq_xvTIq8btIx9sGM deleted.","title":"Delete correction interactively"},{"location":"content/corrections/#delete-correction-with-doc","text":"$ gracc-correct vo delete --doc '{\"VOName\": \"example2\", \"ReportableVOName\": \"example\", \"CorrectedVOName\": \"fermilab\"}' {u'VOName': u'example2', u'ReportableVOName': u'example', u'CorrectedVOName': u'fermilab'} Delete record? (Y/N) y Correction AVyOr3nYTIq8btIx9sGN deleted.","title":"Delete correction with doc"},{"location":"content/corrections/#delete-correction-with-id","text":"$ gracc-correct vo delete --id AVyOwmxFTIq8btIx9sGP {u'VOName': u'example', u'ReportableVOName': u'example', u'CorrectedVOName': u'osg'} Delete record? (Y/N) y Correction AVyOwmxFTIq8btIx9sGP deleted.","title":"Delete correction with id"},{"location":"dev-docs/agent-arch/","text":"Agent Architecture The agents that coordinate for GRACC Unlike its predecessor Gratia, GRACC is split into a number of agents that coordinate through a message queue. The intent is that this separates the distinct components into separate modules that can evolve at independent rates. Further, it provides a mechanism for external entities interested in accounting data to integrate into the system. Components The three major centralized components of GRACC include: Message queue: A RabbitMQ service for exchanging messages between system components. Utilized for its publish-subscribe model and its standardized wire format. GRACC - a centralized collector endpoint. This is a HTTP-based service that listens for incoming records from legacy probes or Gratia collectors and sends them to the message queue. GRACE - an ElasticSearch -based data storage service. Consists of an ElasticSearch database instance and several agents used to populate the system. Other pieces of the accounting infrastructure include the site probes (which produce the records) and planned web views of the accounting data (likely based on Grafana or Kibana). We also plan on developing gracc-replay , a command-line tool for initializing replay of data in the system. This is meant to: Upload Gratia raw record tarballs from disk to the message queue. Request raw data to be resent from a given GRACE instance to a message queue destination (likely a second GRACE instance). Request summary data to be recalculated from a given GRACE instance to a message queue destination. Agents Raw Agent An agent which listens to one or more message queues (typically, its own queue for replay information and one or more collector queues) for raw records. Records are read off the queue and uploaded to the database. Summary Agent This agent has two responsibilities: Listening to a message queue ( /grace.<db>.summary ) for summary records. It fetches the records from the queue and uploads them into ElasticSearch. This is implemented in Logstash. Periodically request new summaries be made by the Listener agent. The current behavior of the summarizer is Every 15 minutes, we re-summarize the past 7 days of data. The summary agent also comes with a command line option to re-summarize larger period of times. The command line is graccsummarizer . The graccsummarizer takes a date range as arguments, further help with the command line can be found with the help option. Listener Agent A agent running on GRACE . The listener agent listens for one-time data replication requests (for either raw or summary data) on the message queue and launches an appropriate sub-process to send the data to the requested destination. It listens on the known queue /gracc.<db>.requests (as defined on Message Queues ). Integrate OIM Information The listener agent integrates OIM information for ProjectNames. For each summary data request it performs these operations: Download the Project information in XML from a OIM URL. Parse the XML Project information into a hash keyed by the ProjectName. For each summary record, search for the project information in the OIM hashed data structure, and append the information to the record. The attributes copied to the record are: PI Name - The name of the principle investigator for the project. Organization - Institution or organization that the project belogs. Department - Department inside the institution. Field Of Science - Field of science inside the Organization. We currently do not support the addition of \"Sponsor Campus Grids\". Future components Components that will likely be needed in the future include: GRACE-B : Listens for raw records and serializes them to disk; on a daily basis, compact them into a tarball and upload them to archival storage. GRACE-D : A dead letter queue : a destination for any unparseable or otherwise-rejected records. Some destination for status information. Every 15 minutes, each component should generate a short status update (analogous to a HTCondor daemon's ClassAd in a condor_collector ) and serialize it to a database.","title":"Agent Architecture"},{"location":"dev-docs/agent-arch/#agent-architecture","text":"The agents that coordinate for GRACC Unlike its predecessor Gratia, GRACC is split into a number of agents that coordinate through a message queue. The intent is that this separates the distinct components into separate modules that can evolve at independent rates. Further, it provides a mechanism for external entities interested in accounting data to integrate into the system.","title":"Agent Architecture"},{"location":"dev-docs/agent-arch/#components","text":"The three major centralized components of GRACC include: Message queue: A RabbitMQ service for exchanging messages between system components. Utilized for its publish-subscribe model and its standardized wire format. GRACC - a centralized collector endpoint. This is a HTTP-based service that listens for incoming records from legacy probes or Gratia collectors and sends them to the message queue. GRACE - an ElasticSearch -based data storage service. Consists of an ElasticSearch database instance and several agents used to populate the system. Other pieces of the accounting infrastructure include the site probes (which produce the records) and planned web views of the accounting data (likely based on Grafana or Kibana). We also plan on developing gracc-replay , a command-line tool for initializing replay of data in the system. This is meant to: Upload Gratia raw record tarballs from disk to the message queue. Request raw data to be resent from a given GRACE instance to a message queue destination (likely a second GRACE instance). Request summary data to be recalculated from a given GRACE instance to a message queue destination.","title":"Components"},{"location":"dev-docs/agent-arch/#agents","text":"","title":"Agents"},{"location":"dev-docs/agent-arch/#raw-agent","text":"An agent which listens to one or more message queues (typically, its own queue for replay information and one or more collector queues) for raw records. Records are read off the queue and uploaded to the database.","title":"Raw Agent"},{"location":"dev-docs/agent-arch/#summary-agent","text":"This agent has two responsibilities: Listening to a message queue ( /grace.<db>.summary ) for summary records. It fetches the records from the queue and uploads them into ElasticSearch. This is implemented in Logstash. Periodically request new summaries be made by the Listener agent. The current behavior of the summarizer is Every 15 minutes, we re-summarize the past 7 days of data. The summary agent also comes with a command line option to re-summarize larger period of times. The command line is graccsummarizer . The graccsummarizer takes a date range as arguments, further help with the command line can be found with the help option.","title":"Summary Agent"},{"location":"dev-docs/agent-arch/#listener-agent","text":"A agent running on GRACE . The listener agent listens for one-time data replication requests (for either raw or summary data) on the message queue and launches an appropriate sub-process to send the data to the requested destination. It listens on the known queue /gracc.<db>.requests (as defined on Message Queues ).","title":"Listener Agent"},{"location":"dev-docs/agent-arch/#integrate-oim-information","text":"The listener agent integrates OIM information for ProjectNames. For each summary data request it performs these operations: Download the Project information in XML from a OIM URL. Parse the XML Project information into a hash keyed by the ProjectName. For each summary record, search for the project information in the OIM hashed data structure, and append the information to the record. The attributes copied to the record are: PI Name - The name of the principle investigator for the project. Organization - Institution or organization that the project belogs. Department - Department inside the institution. Field Of Science - Field of science inside the Organization. We currently do not support the addition of \"Sponsor Campus Grids\".","title":"Integrate OIM Information"},{"location":"dev-docs/agent-arch/#future-components","text":"Components that will likely be needed in the future include: GRACE-B : Listens for raw records and serializes them to disk; on a daily basis, compact them into a tarball and upload them to archival storage. GRACE-D : A dead letter queue : a destination for any unparseable or otherwise-rejected records. Some destination for status information. Every 15 minutes, each component should generate a short status update (analogous to a HTCondor daemon's ClassAd in a condor_collector ) and serialize it to a database.","title":"Future components"},{"location":"dev-docs/backups/","text":"Backup Configuration Backup Sources GRACC backup sources come from listening and duplicating all raw records sent to the system through the RabbitMQ system. The GRACC Archiver listens to the raw RabbitMQ exchanges. It listens to both the gracc.osg.raw and the gracc.osg-transfer.raw exchanges. The archive agent stores the records into a tar.gz file located in /var/lib/graccarchive/sandbox . On new days (or agent crashes) the tar.gz files are atomically copied to /var/lib/graccarchive/output . The transfer archiver similarily stores files in /var/lib/graccarchive/sandbox-transfer and /var/lib/graccarchive/output-transfer . The archive agent is configured in /etc/graccarchive/conf . It uses systemd template units to run both the raw jobs and raw transfer archivers at the same time. Backup Location The backups are copied to FNAL by the gracc-backup tool. This uses SystemD timer and service files to periodically copy the output .tar.gz files to FNAL. The final destination (gsiftp) of the files is configured in the *.service files with the tool Restore Operation The restore operation uses the graccunarchiver tool distributed with the GRACC Archiver agent. The workflow of a restore is: Copy the backup file from the backup location. You will likely need to use globus-url-copy in order to copy the files back from the backup location. Run the graccunarchiver tool from the GRACC Archiver on the compressed .tar.gz file, with command line arguments for the RabbitMQ parameters. graccunarchiver <rabbitmq_url> gracc.osg.raw gracc-2017-04-04.tar.gz After restoring the raw jobs and transfers, it may be necessary to re-summarize the restored time-period with the graccsummarizer tool. Note If the backup tar.gz file was created during a crash of the agent or system, it's possible that the tar.gz end may be corrupted and you may see CRC or other errors. The vast majority of the records are fine, but the last few may be corrupted and un-retrievable.","title":"Backup Documentation"},{"location":"dev-docs/backups/#backup-configuration","text":"","title":"Backup Configuration"},{"location":"dev-docs/backups/#backup-sources","text":"GRACC backup sources come from listening and duplicating all raw records sent to the system through the RabbitMQ system. The GRACC Archiver listens to the raw RabbitMQ exchanges. It listens to both the gracc.osg.raw and the gracc.osg-transfer.raw exchanges. The archive agent stores the records into a tar.gz file located in /var/lib/graccarchive/sandbox . On new days (or agent crashes) the tar.gz files are atomically copied to /var/lib/graccarchive/output . The transfer archiver similarily stores files in /var/lib/graccarchive/sandbox-transfer and /var/lib/graccarchive/output-transfer . The archive agent is configured in /etc/graccarchive/conf . It uses systemd template units to run both the raw jobs and raw transfer archivers at the same time.","title":"Backup Sources"},{"location":"dev-docs/backups/#backup-location","text":"The backups are copied to FNAL by the gracc-backup tool. This uses SystemD timer and service files to periodically copy the output .tar.gz files to FNAL. The final destination (gsiftp) of the files is configured in the *.service files with the tool","title":"Backup Location"},{"location":"dev-docs/backups/#restore-operation","text":"The restore operation uses the graccunarchiver tool distributed with the GRACC Archiver agent. The workflow of a restore is: Copy the backup file from the backup location. You will likely need to use globus-url-copy in order to copy the files back from the backup location. Run the graccunarchiver tool from the GRACC Archiver on the compressed .tar.gz file, with command line arguments for the RabbitMQ parameters. graccunarchiver <rabbitmq_url> gracc.osg.raw gracc-2017-04-04.tar.gz After restoring the raw jobs and transfers, it may be necessary to re-summarize the restored time-period with the graccsummarizer tool. Note If the backup tar.gz file was created during a crash of the agent or system, it's possible that the tar.gz end may be corrupted and you may see CRC or other errors. The vast majority of the records are fine, but the last few may be corrupted and un-retrievable.","title":"Restore Operation"},{"location":"dev-docs/gracc-reporting/","text":"GRACC Reporting gracc-reporting is a python package (python 2.7+) that consists of a standard set of reporting libraries, and individual report executables that query GRACC, aggregate the data, and send reports to various stakeholders. A lot of this information has been pulled from (and updated since then) this presentation . Reports in gracc-reporting There are currently three subpackages in gracc-reporting. In the future, we plan to split these out from the main set of reporting libraries. These packages are: OSG Reports OSG Flocking Report (osgflockingreport) - VO usage of OSG sites reported by flocking probes OSG Project Usage Report (osgreport) - Project usage of OSG resources (OSG-Direct, OSG-Connect, XD) OSG Missing Projects from Records Report (osgmissingprojects) OSG VO Usage Per Site Report (osgpersitereport) Top [N] Providers of Opportunistic Hours on the OSG (News Report) (osgtopoppusagereport) Gratia Probes that haven't reported in the past two days (osgprobereport) FIFE Reports Job Success Rate Report (jobsuccessratereport) User Efficiency Report (efficiencyreport) Top Wasted Hours by Users by VO Report (topwastedhoursvoreport) Minerva Report (deprecated) Source The source code for gracc-reporting is on the OSG's Github Page . The Dockerfiles are contained in that repository as well, but the deployment structure using docker is detailed on the Services page. gracc-reporting structure General Report Structure Each report is subclass of ReportUtils.Reporter Implemented methods in most reports: query: Define Elasticsearch query ReportUtils.run_query: Run that query and check for errors before proceeding generate: Take results from query and parse them to generate raw data for any further processing format_report (sometimes): Take processed data (sometimes from generate, sometimes from intermediate methods), put it into report format for TextUtils module to generate HTML, CSV, plain text. If this isn\u2019t used, the formatting is done elsewhere in the report code. For example, more complex HTML templates send_report: Email the report and check for errors run_report: Run all of the above. Mostly useful for more complex use cases (Probe Report, OSG Usage Per Site Report Running Reports in Development Each report has associated executable. All can be used with -h, --help flag for full list of options Generally, something like: osgflockingreport -s \u201c2016-11-09 06:00:00\u201d -e \u201c2016-11-16 06:00:00\u201d Notes: Almost all reports require a start time (-s) and end time (-e) Times are assumed to be local time of host running reports. The reports will convert to UTC to query GRACC. Useful flags for all reports: -d: dryrun (email only goes to test_to_emails in config file) -n: no email sent (unless there\u2019s an error. Then emails go to dryrun folks) -v: verbose Configuration files Configuration files are simple toml files that are read through the toml python module. There is one for each subpackage. In keeping with setuptools' philosophy, the config files and html templates that are shipped with the source code are kept within the package at src/graccreports/{config|html_templates} . In each config file, the report sections correspond to a ReportUtils.Reporter.report_type . Future deprecated behavior: Currently, the common reporting libraries (in this case, ReportUtils.py) automatically look for the config file in the following places (in order): * Override (see below ) * /etc/gracc-reporting/{config|html_templates} * Within package using pkg_resources The plan is to change this in the future to look for the config file in an override, an environment variable, and then within the package. /etc/gracc-reporting might be kept if the deployment model is changed. Similar to config files, the reports automatically try to write logs to the following locations: * Override * Specified in config file * /var/log/gracc-reporting * $HOME/gracc-reporting * /tmp/gracc-reporting This will also be changed, so that the following order is observed: override, environment variable, config file, $HOME/gracc-reporting . In practice, the others are good, but unnecessary. Override flags: -c: Config file -T: HTML Template -L: logfile Installation Setuptools Clone repository: git clone https://github.com/opensciencegrid/gracc-reporting pip install -r requirements.txt python setup.py install After this is complete, remember to change the config files within the package (or wherever you copy them) as needed (email recipients, etc.) RPM (future) RPM will be available online Currently use this inside docker image Config files, HTML templates, etc. installed at /etc/gracc-reporting/ If you\u2019re running RHEL7, CentOS7, SL7, etc., bug in package python-urllib3 in primary repo (RPM installation) - breaks python-elasticsearch In my docker images, I uninstall python-urllib3, and pip install urllib3 to get around that (PyPI version is fine) Addendum: More detailed developer notes Shared Libraries ReportUtils.py: The big one Reporter class: Establish Elasticsearch client Set correct ES index pattern (rather than just using gracc.osg.raw-* , unless we really want to query all raw records) Run query with or without aggregations Send simpler reports Set up logging, get email info from config files, parse options to reports Also has functions to find config, HTML template files, as well as handle errors IndexPattern.py: Figures out index pattern given a date range and flags. Assumes gracc.osg.summary if there's no config file index_pattern flag for a given report. TextUtils.py: Used by ReportUtils to generate reports for simpler HTML templates TimeUtils.py: Datetime validation and time zone handling (built heavily off of datetime and dateutil, creates timestamps for grafana) Building a new release Implemented methods in most reports: Edit setup.py if needed (if you have a new report, add executable, version update, etc.) python setup.py sdist Python package: Copy out the tarball in dist/ wherever you want to deploy, or push it to github and pull from there RPM: Spec file is in config/ (symlink to src/graccreports/config/ )","title":"GRACC Reporting"},{"location":"dev-docs/gracc-reporting/#gracc-reporting","text":"gracc-reporting is a python package (python 2.7+) that consists of a standard set of reporting libraries, and individual report executables that query GRACC, aggregate the data, and send reports to various stakeholders. A lot of this information has been pulled from (and updated since then) this presentation .","title":"GRACC Reporting"},{"location":"dev-docs/gracc-reporting/#reports-in-gracc-reporting","text":"There are currently three subpackages in gracc-reporting. In the future, we plan to split these out from the main set of reporting libraries. These packages are: OSG Reports OSG Flocking Report (osgflockingreport) - VO usage of OSG sites reported by flocking probes OSG Project Usage Report (osgreport) - Project usage of OSG resources (OSG-Direct, OSG-Connect, XD) OSG Missing Projects from Records Report (osgmissingprojects) OSG VO Usage Per Site Report (osgpersitereport) Top [N] Providers of Opportunistic Hours on the OSG (News Report) (osgtopoppusagereport) Gratia Probes that haven't reported in the past two days (osgprobereport) FIFE Reports Job Success Rate Report (jobsuccessratereport) User Efficiency Report (efficiencyreport) Top Wasted Hours by Users by VO Report (topwastedhoursvoreport) Minerva Report (deprecated)","title":"Reports in gracc-reporting"},{"location":"dev-docs/gracc-reporting/#source","text":"The source code for gracc-reporting is on the OSG's Github Page . The Dockerfiles are contained in that repository as well, but the deployment structure using docker is detailed on the Services page.","title":"Source"},{"location":"dev-docs/gracc-reporting/#gracc-reporting-structure","text":"","title":"gracc-reporting structure"},{"location":"dev-docs/gracc-reporting/#general-report-structure","text":"Each report is subclass of ReportUtils.Reporter Implemented methods in most reports: query: Define Elasticsearch query ReportUtils.run_query: Run that query and check for errors before proceeding generate: Take results from query and parse them to generate raw data for any further processing format_report (sometimes): Take processed data (sometimes from generate, sometimes from intermediate methods), put it into report format for TextUtils module to generate HTML, CSV, plain text. If this isn\u2019t used, the formatting is done elsewhere in the report code. For example, more complex HTML templates send_report: Email the report and check for errors run_report: Run all of the above. Mostly useful for more complex use cases (Probe Report, OSG Usage Per Site Report","title":"General Report Structure"},{"location":"dev-docs/gracc-reporting/#running-reports-in-development","text":"Each report has associated executable. All can be used with -h, --help flag for full list of options Generally, something like: osgflockingreport -s \u201c2016-11-09 06:00:00\u201d -e \u201c2016-11-16 06:00:00\u201d Notes: Almost all reports require a start time (-s) and end time (-e) Times are assumed to be local time of host running reports. The reports will convert to UTC to query GRACC. Useful flags for all reports: -d: dryrun (email only goes to test_to_emails in config file) -n: no email sent (unless there\u2019s an error. Then emails go to dryrun folks) -v: verbose","title":"Running Reports in Development"},{"location":"dev-docs/gracc-reporting/#configuration-files","text":"Configuration files are simple toml files that are read through the toml python module. There is one for each subpackage. In keeping with setuptools' philosophy, the config files and html templates that are shipped with the source code are kept within the package at src/graccreports/{config|html_templates} . In each config file, the report sections correspond to a ReportUtils.Reporter.report_type .","title":"Configuration files"},{"location":"dev-docs/gracc-reporting/#future-deprecated-behavior","text":"Currently, the common reporting libraries (in this case, ReportUtils.py) automatically look for the config file in the following places (in order): * Override (see below ) * /etc/gracc-reporting/{config|html_templates} * Within package using pkg_resources The plan is to change this in the future to look for the config file in an override, an environment variable, and then within the package. /etc/gracc-reporting might be kept if the deployment model is changed. Similar to config files, the reports automatically try to write logs to the following locations: * Override * Specified in config file * /var/log/gracc-reporting * $HOME/gracc-reporting * /tmp/gracc-reporting This will also be changed, so that the following order is observed: override, environment variable, config file, $HOME/gracc-reporting . In practice, the others are good, but unnecessary.","title":"Future deprecated behavior:"},{"location":"dev-docs/gracc-reporting/#override-flags","text":"-c: Config file -T: HTML Template -L: logfile","title":"Override flags:"},{"location":"dev-docs/gracc-reporting/#installation","text":"","title":"Installation"},{"location":"dev-docs/gracc-reporting/#setuptools","text":"Clone repository: git clone https://github.com/opensciencegrid/gracc-reporting pip install -r requirements.txt python setup.py install After this is complete, remember to change the config files within the package (or wherever you copy them) as needed (email recipients, etc.)","title":"Setuptools"},{"location":"dev-docs/gracc-reporting/#rpm-future","text":"RPM will be available online Currently use this inside docker image Config files, HTML templates, etc. installed at /etc/gracc-reporting/ If you\u2019re running RHEL7, CentOS7, SL7, etc., bug in package python-urllib3 in primary repo (RPM installation) - breaks python-elasticsearch In my docker images, I uninstall python-urllib3, and pip install urllib3 to get around that (PyPI version is fine)","title":"RPM (future)"},{"location":"dev-docs/gracc-reporting/#addendum-more-detailed-developer-notes","text":"","title":"Addendum:  More detailed developer notes"},{"location":"dev-docs/gracc-reporting/#shared-libraries","text":"ReportUtils.py: The big one Reporter class: Establish Elasticsearch client Set correct ES index pattern (rather than just using gracc.osg.raw-* , unless we really want to query all raw records) Run query with or without aggregations Send simpler reports Set up logging, get email info from config files, parse options to reports Also has functions to find config, HTML template files, as well as handle errors IndexPattern.py: Figures out index pattern given a date range and flags. Assumes gracc.osg.summary if there's no config file index_pattern flag for a given report. TextUtils.py: Used by ReportUtils to generate reports for simpler HTML templates TimeUtils.py: Datetime validation and time zone handling (built heavily off of datetime and dateutil, creates timestamps for grafana)","title":"Shared Libraries"},{"location":"dev-docs/gracc-reporting/#building-a-new-release","text":"Implemented methods in most reports: Edit setup.py if needed (if you have a new report, add executable, version update, etc.) python setup.py sdist Python package: Copy out the tarball in dist/ wherever you want to deploy, or push it to github and pull from there RPM: Spec file is in config/ (symlink to src/graccreports/config/ )","title":"Building a new release"},{"location":"dev-docs/install-grace-db/","text":"Installing GRACE The GRACE database service consists of: ElasticSearch as a datastore. grace-raw-listener : Listens to the raw records from the collector grace-summary-listener : Listens for and requests summary records. grace-request-listener : Listens for replay and summarization requests. See the agent architecture docs for more information. Additionally, for monitoring and visualization, one commonly installs the following external components. InfluxDB (primarily for monitoring ES performance) Grafana (visualization) Kibana (visualization) Dependencies This document assumes a RHEL7 host with ElasticSearch pre-installed and functioning. We also assume that the OSG repos and yum priorities have been appropriately configured . Installation The relevant components can be pulled in via a meta-RPM: yum install --enablerepo=osg-development osg-grace Configuration Configuration files are kept in /etc/gracc/config.d and /usr/share/gracc/config.d ; files in this directory are read in lexigraphical order. The file format is TOML . You will likely need to override at least the AMQP connection paramaters (username and password). Do not edit the default file in /usr/share/gracc/config.d : these will be overwritten on upgrade. Instead, start by editing the samples in /etc . Most strings will expand the text %I to the \"instance name\". So, for the osg instance, the following, [AMQP] exchange = \"gracc.%I.raw\" queue = \"grace.%I.raw\" is equivalent to [AMQP] exchange = \"gracc.osg.raw\" queue = \"grace.osg.raw\" Further, we can have specific instance overrides. Hence, [AMQP] exchange = \"gracc.%I.raw\" queue = \"grace.%I.raw\" [AMQP.osg] exchange = \"gracc.osg-test.raw\" is equivalent to: [AMQP] exchange = \"gracc.osg-test.raw\" queue = \"gracc.osg.raw\" Running services To configure GRACC to start at boot, you will need to do the following for the osg instance: ln -sf /usr/lib/systemd/system/grace-raw-listener@.service /etc/systemd/system/multi-user.target.wants/grace-raw-listener@osg.service ln -sf /usr/lib/systemd/system/grace-summary-listener@.service /etc/systemd/system/multi-user.target.wants/grace-summary-listener@osg.service ln -sf /usr/lib/systemd/system/grace-request-listener@.service /etc/systemd/system/multi-user.target.wants/grace-request-listener@osg.service systemctl daemon-reload Multiple instance can be run by editing the instance name above. Finally, these services can be started via the typical system management commands: systemctl start grace-raw-listener@osg systemctl start grace-summary-listener@osg systemctl start grace-request-listener@osg Log files By default, log files go into /var/log/gracc .","title":"Installation"},{"location":"dev-docs/install-grace-db/#installing-grace","text":"The GRACE database service consists of: ElasticSearch as a datastore. grace-raw-listener : Listens to the raw records from the collector grace-summary-listener : Listens for and requests summary records. grace-request-listener : Listens for replay and summarization requests. See the agent architecture docs for more information. Additionally, for monitoring and visualization, one commonly installs the following external components. InfluxDB (primarily for monitoring ES performance) Grafana (visualization) Kibana (visualization)","title":"Installing GRACE"},{"location":"dev-docs/install-grace-db/#dependencies","text":"This document assumes a RHEL7 host with ElasticSearch pre-installed and functioning. We also assume that the OSG repos and yum priorities have been appropriately configured .","title":"Dependencies"},{"location":"dev-docs/install-grace-db/#installation","text":"The relevant components can be pulled in via a meta-RPM: yum install --enablerepo=osg-development osg-grace","title":"Installation"},{"location":"dev-docs/install-grace-db/#configuration","text":"Configuration files are kept in /etc/gracc/config.d and /usr/share/gracc/config.d ; files in this directory are read in lexigraphical order. The file format is TOML . You will likely need to override at least the AMQP connection paramaters (username and password). Do not edit the default file in /usr/share/gracc/config.d : these will be overwritten on upgrade. Instead, start by editing the samples in /etc . Most strings will expand the text %I to the \"instance name\". So, for the osg instance, the following, [AMQP] exchange = \"gracc.%I.raw\" queue = \"grace.%I.raw\" is equivalent to [AMQP] exchange = \"gracc.osg.raw\" queue = \"grace.osg.raw\" Further, we can have specific instance overrides. Hence, [AMQP] exchange = \"gracc.%I.raw\" queue = \"grace.%I.raw\" [AMQP.osg] exchange = \"gracc.osg-test.raw\" is equivalent to: [AMQP] exchange = \"gracc.osg-test.raw\" queue = \"gracc.osg.raw\"","title":"Configuration"},{"location":"dev-docs/install-grace-db/#running-services","text":"To configure GRACC to start at boot, you will need to do the following for the osg instance: ln -sf /usr/lib/systemd/system/grace-raw-listener@.service /etc/systemd/system/multi-user.target.wants/grace-raw-listener@osg.service ln -sf /usr/lib/systemd/system/grace-summary-listener@.service /etc/systemd/system/multi-user.target.wants/grace-summary-listener@osg.service ln -sf /usr/lib/systemd/system/grace-request-listener@.service /etc/systemd/system/multi-user.target.wants/grace-request-listener@osg.service systemctl daemon-reload Multiple instance can be run by editing the instance name above. Finally, these services can be started via the typical system management commands: systemctl start grace-raw-listener@osg systemctl start grace-summary-listener@osg systemctl start grace-request-listener@osg","title":"Running services"},{"location":"dev-docs/install-grace-db/#log-files","text":"By default, log files go into /var/log/gracc .","title":"Log files"},{"location":"dev-docs/message-queues/","text":"Message Queues Message queues used in GRACC In AMQP, there is a difference between a queue and an exchange . Messages delivered on a queue are read by a single subscriber; messages delivered on an exchange will be delivered to all subscribers (implying they may be buffered for some time at the broker if a given client goes missing). We would like collectors to serve multiple databases (hence the use of an exchange ) while queues are used for messages sent to a database agent. Well known message queues and exchanges used: /gracc.<collector>.raw - An exchange which listens to raw records to insert into the collector. This is the interface that probes would send raw records. /grace.<db>.summary - A queue that listens for summary records to insert into a specific <db> . This is used to replicate summary records from other collectors or db's. /grace.<db>.raw - Raw record queue for a database instance. /grace.<db>.requests - The Ad Agent listens to this queue for requests for raw and summary replications. Here, <db> is the instance name of a given database install while <collector> is the instance name of an existing Gratia collector. There are currently three defined message schemas in GRACC: raw records, summary records, and replay requests: Raw Records These are JSON-formatted documents; the key-value pairs are derived from the OGF UsageRecord format. For ease of compatibility with the prior Gratia system, we include an njobs attribute if a given record represents more than one job. The Raw Records page has more details and the mapping from XML UsageRecord. { \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\", \"CreateTime\": \"2016-05-27T22:46:46Z\", \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\", \"LocalJobId\": \"185777\", \"LocalUserId\": \"cmsuser\", \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\", \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=cmsuser/CN=1234567/CN=CMS User\", \"VOName\": \"/cms/Role=production/Capability=NULL\", \"ReportableVOName\": \"cms\", \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\", \"MachineName\": \"osg-gw-7.t2.ucsd.edu\", \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\", \"Status\": \"0\", \"Status_description\": \"Condor Exit Status\", \"WallDuration\": 617, \"CpuDuration\": 18, \"CpuDuration_system\": 18, \"CpuDuration_user\": 0, \"EndTime\": \"2016-05-27T22:44:08Z\", \"StartTime\": \"2016-05-27T22:33:51Z\", \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\", \"NodeCount\": \"1\", \"NodeCount_metric\": \"max\", \"Processors\": \"1\", \"Processors_metric\": \"max\", \"ResourceType\": \"Batch\", \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\", \"SiteName\": \"UCSDT2-D\", \"Grid\": \"OSG\", \"Njobs\": \"1\", } Note We consider these to be \"base\" keys: additional ones may be given (for example, if the record is derived from a HTCondor ClassAd). Summary Records The summary record represents a grouping of multiple similar raw records. In GRACC, we often group jobs run on the same date, by the same user, on the same resource. TODO: copy JSON document here Replay Requests The replay request indicates that a remote listener agent attached to an ElasticSearch database should load and re-send some amount of data. Keys : from and to : An ISO 8601 formatted date & time string that determines the time range beginning and ending, respectively, of the data to be sent. kind : What type of records should be resent (valid values are curently raw or summary ). destination : An exchange on the same broker where records should be sent. Should be a string value. routing_key : A routing key to be used when sending the data control and control_key : (optional) Control channel that will be notified when the data stream starts and ends. Further, it will receive any errors that may occur during the replay. filter : (not implemented) A ElasticSearch-formatted query filter (JSON value). Only records matching this filter should be sent. Example { \"from\": \"2016-05-10T00:00:00\", \"to\": \"2016-05-11T00:00:00\", \"kind\": \"raw\", \"destination\": \"grace.osg.raw\", \"routing_key\": \"grace.osg.raw\", \"control\": \"control-exchange\", \"control_key\": \"control_routing_key\", \"filter\": { \"query\": { \"query_string\": { \"query\": \"vo=cms\" } } } }","title":"Message Queues"},{"location":"dev-docs/message-queues/#message-queues","text":"Message queues used in GRACC In AMQP, there is a difference between a queue and an exchange . Messages delivered on a queue are read by a single subscriber; messages delivered on an exchange will be delivered to all subscribers (implying they may be buffered for some time at the broker if a given client goes missing). We would like collectors to serve multiple databases (hence the use of an exchange ) while queues are used for messages sent to a database agent. Well known message queues and exchanges used: /gracc.<collector>.raw - An exchange which listens to raw records to insert into the collector. This is the interface that probes would send raw records. /grace.<db>.summary - A queue that listens for summary records to insert into a specific <db> . This is used to replicate summary records from other collectors or db's. /grace.<db>.raw - Raw record queue for a database instance. /grace.<db>.requests - The Ad Agent listens to this queue for requests for raw and summary replications. Here, <db> is the instance name of a given database install while <collector> is the instance name of an existing Gratia collector. There are currently three defined message schemas in GRACC: raw records, summary records, and replay requests:","title":"Message Queues"},{"location":"dev-docs/message-queues/#raw-records","text":"These are JSON-formatted documents; the key-value pairs are derived from the OGF UsageRecord format. For ease of compatibility with the prior Gratia system, we include an njobs attribute if a given record represents more than one job. The Raw Records page has more details and the mapping from XML UsageRecord. { \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\", \"CreateTime\": \"2016-05-27T22:46:46Z\", \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\", \"LocalJobId\": \"185777\", \"LocalUserId\": \"cmsuser\", \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\", \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=cmsuser/CN=1234567/CN=CMS User\", \"VOName\": \"/cms/Role=production/Capability=NULL\", \"ReportableVOName\": \"cms\", \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\", \"MachineName\": \"osg-gw-7.t2.ucsd.edu\", \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\", \"Status\": \"0\", \"Status_description\": \"Condor Exit Status\", \"WallDuration\": 617, \"CpuDuration\": 18, \"CpuDuration_system\": 18, \"CpuDuration_user\": 0, \"EndTime\": \"2016-05-27T22:44:08Z\", \"StartTime\": \"2016-05-27T22:33:51Z\", \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\", \"NodeCount\": \"1\", \"NodeCount_metric\": \"max\", \"Processors\": \"1\", \"Processors_metric\": \"max\", \"ResourceType\": \"Batch\", \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\", \"SiteName\": \"UCSDT2-D\", \"Grid\": \"OSG\", \"Njobs\": \"1\", } Note We consider these to be \"base\" keys: additional ones may be given (for example, if the record is derived from a HTCondor ClassAd).","title":"Raw Records"},{"location":"dev-docs/message-queues/#summary-records","text":"The summary record represents a grouping of multiple similar raw records. In GRACC, we often group jobs run on the same date, by the same user, on the same resource. TODO: copy JSON document here","title":"Summary Records"},{"location":"dev-docs/message-queues/#replay-requests","text":"The replay request indicates that a remote listener agent attached to an ElasticSearch database should load and re-send some amount of data. Keys : from and to : An ISO 8601 formatted date & time string that determines the time range beginning and ending, respectively, of the data to be sent. kind : What type of records should be resent (valid values are curently raw or summary ). destination : An exchange on the same broker where records should be sent. Should be a string value. routing_key : A routing key to be used when sending the data control and control_key : (optional) Control channel that will be notified when the data stream starts and ends. Further, it will receive any errors that may occur during the replay. filter : (not implemented) A ElasticSearch-formatted query filter (JSON value). Only records matching this filter should be sent. Example { \"from\": \"2016-05-10T00:00:00\", \"to\": \"2016-05-11T00:00:00\", \"kind\": \"raw\", \"destination\": \"grace.osg.raw\", \"routing_key\": \"grace.osg.raw\", \"control\": \"control-exchange\", \"control_key\": \"control_routing_key\", \"filter\": { \"query\": { \"query_string\": { \"query\": \"vo=cms\" } } } }","title":"Replay Requests"},{"location":"dev-docs/raw-records/","text":"Raw Records Raw records are in JSON format with a schema derived from the OGF UsageRecord specification used by GRACC's predecessor Gratia. Requirements To maintain flexibility and fully leverage the schemaless storage being used, the schema requirements are kept to a minimum. Some fields are expected by GRACC compenents so leaving them out will result in the records not being properly accounted: ResourceType CommonName VOName ReportableVOName ProjectName EndTime CpuDuration WallDuration Processors ...? Dates and Times All times are strings in ISO8601 format. Time durations are floats representing seconds. Converting XML JobUsageRecord The mapping from an XML JobUsageRecord to a JSON Raw GRACC record is outlined below; this should help inform how new records are generated as well. The raw XML record is stored in the RawXML field, to allow for later reference and remapping. Identity Groups Identity groups are flattened by moving their sub-elements to the top level: RecordIdentity RecordId CreateTime JobIdentity GlobalJobId LocalJobId ProcessId (array) UserIdentity LocalUserId GlobalUsername CommonName DN VOName ReportableVOName Durations Duration fields are converted to seconds. CpuDuration can have usage \"user\" or \"system\", these are also moved into the top level: CpuDuration (combined) CpuDuration_user CpuDuration_system WallDuration Resource Resources are transformed into a <description>:<value> map. The description is transformed to make it a valid field name (spaces and dots are converted to dashes) prefixed with Resource_ . Other properties are flattened as Resource_<description>_<property_name>:<property_value> . The special Resource ResourceType is moved directly to the top level. ResourceType BatchPilot is renamed Payload due to the former being misleading. TimeDuration and TimeInstant elements are likewise put in <type>:<value> maps with their respective prefixes. Durations are converted to seconds, discrete times are ISO8601 strings. Other Any other elements are directly included in the top level. Properties of those elements are moved to fields named as <element>_<property> , e.g. JobName_description . Example XML Record <JobUsageRecord xmlns=\"http://www.gridforum.org/2003/ur-wg\" xmlns:urwg=\"http://www.gridforum.org/2003/ur-wg\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gridforum.org/2003/ur-wg file:///u:/OSG/urwg-schema.11.xsd\"> <RecordIdentity urwg:createTime=\"2016-05-27T22:46:46Z\" urwg:recordId=\"osg-gw-7.t2.ucsd.edu:35741.2\"/> <JobIdentity> <GlobalJobId>condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242</GlobalJobId> <LocalJobId>185777</LocalJobId> </JobIdentity> <UserIdentity> <LocalUserId>cmsuser</LocalUserId> <GlobalUsername>cmsuser@t2.ucsd.edu</GlobalUsername> <DN>/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba</DN> <VOName>/cms/Role=production/Capability=NULL</VOName> <ReportableVOName>cms</ReportableVOName> </UserIdentity> <JobName>osg-gw-7.t2.ucsd.edu#185777.0#1464388242</JobName> <MachineName>osg-gw-7.t2.ucsd.edu</MachineName> <SubmitHost>osg-gw-7.t2.ucsd.edu</SubmitHost> <Status urwg:description=\"Condor Exit Status\">0</Status> <WallDuration urwg:description=\"Was entered in seconds\">PT10M17.0S</WallDuration> <TimeDuration urwg:type=\"RemoteUserCpu\">PT0S</TimeDuration> <TimeD <TimeDuration urwg:type=\"RemoteSysCpu\">PT18.0S</TimeDuration> <TimeDuration urwg:type=\"LocalSysCpu\">PT0S</TimeDuration> <TimeDuration urwg:type=\"CumulativeSuspensionTime\">PT0S</TimeDuration> <TimeDuration urwg:type=\"CommittedSuspensionTime\">PT0S</TimeDuration> <TimeDuration urwg:type=\"CommittedTime\">PT10M17.0S</TimeDuration> <CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"system\">PT18.0S</CpuDuration> <CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"user\">PT0S</CpuDuration> <EndTime urwg:description=\"Was entered in seconds\">2016-05-27T22:44:08Z</EndTime> <StartTime urwg:description=\"Was entered in seconds\">2016-05-27T22:33:51Z</StartTime> <Host primary=\"true\">cabinet-1-1-1.t2.ucsd.edu</Host> <Queue urwg:description=\"Condor's JobUniverse field\">5</Queue> <NodeCount urwg:metric=\"max\">1</NodeCount> <Processors urwg:metric=\"max\">1</Processors> <Resource urwg:description=\"CondorMyType\">Job</Resource> <Resource urwg:description=\"AccountingGroup\">group_cmsprod.cmsuser</Resource> <Resource urwg:description=\"ExitBySignal\">false</Resource> <Resource urwg:description=\"ExitCode\">0</Resource> <Resource urwg:description=\"condor.JobStatus\">4</Resource> <Network urwg:metric=\"total\" urwg:phaseUnit=\"PT10M17.0S\" urwg:storageUnit=\"b\">0</Network> <ProbeName>condor:osg-gw-7.t2.ucsd.edu</ProbeName> <SiteName>UCSDT2-D</SiteName> <Grid>OSG</Grid> <Njobs>1</Njobs> <Resource urwg:description=\"ResourceType\">Batch</Resource> </JobUsageRecord> Example Corresponding JSON { \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\", \"CreateTime\": \"2016-05-27T22:46:46Z\", \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\", \"LocalJobId\": \"185777\", \"LocalUserId\": \"cmsuser\", \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\", \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\", \"VOName\": \"/cms/Role=production/Capability=NULL\", \"ReportableVOName\": \"cms\", \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\", \"MachineName\": \"osg-gw-7.t2.ucsd.edu\", \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\", \"Status\": \"0\", \"Status_description\": \"Condor Exit Status\", \"WallDuration\": 617, \"WallDuration_description\": \"Was entered in seconds\" \"TimeDuration_CommittedSuspensionTime\": 0, \"TimeDuration_CommittedTime\": 617, \"TimeDuration_CumulativeSuspensionTime\": 0, \"TimeDuration_LocalSysCpu\": 0, \"TimeDuration_LocalUserCpu\": 0, \"TimeDuration_RemoteSysCpu\": 18, \"TimeDuration_RemoteUserCpu\": 0, \"CpuDuration\": 18, \"CpuDuration_system\": 18, \"CpuDuration_system_description\": \"Was entered in seconds\", \"CpuDuration_user\": 0, \"CpuDuration_user_description\": \"Was entered in seconds\", \"EndTime\": \"2016-05-27T22:44:08Z\", \"StartTime\": \"2016-05-27T22:33:51Z\", \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\", \"Queue\": \"5\", \"Queue_description\": \"Condor's JobUniverse field\", \"NodeCount\": \"1\", \"NodeCount_metric\": \"max\", \"Processors\": \"1\", \"Processors_metric\": \"max\", \"Resource_AccountingGroup\": \"group_cmsprod.cmsuser\", \"Resource_CondorMyType\": \"Job\", \"Resource_ExitBySignal\": \"false\", \"Resource_ExitCode\": \"0\", \"Resource_condor-JobStatus\": \"4\", \"ResourceType\": \"Batch\", \"Network\": \"0\", \"Network_metric\": \"total\", \"Network_phaseUnit\": 617, \"Network_storageUnit\": \"b\", \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\", \"SiteName\": \"UCSDT2-D\", \"Grid\": \"OSG\", \"Njobs\": \"1\", }","title":"Raw Records"},{"location":"dev-docs/raw-records/#raw-records","text":"Raw records are in JSON format with a schema derived from the OGF UsageRecord specification used by GRACC's predecessor Gratia.","title":"Raw Records"},{"location":"dev-docs/raw-records/#requirements","text":"To maintain flexibility and fully leverage the schemaless storage being used, the schema requirements are kept to a minimum. Some fields are expected by GRACC compenents so leaving them out will result in the records not being properly accounted: ResourceType CommonName VOName ReportableVOName ProjectName EndTime CpuDuration WallDuration Processors ...?","title":"Requirements"},{"location":"dev-docs/raw-records/#dates-and-times","text":"All times are strings in ISO8601 format. Time durations are floats representing seconds.","title":"Dates and Times"},{"location":"dev-docs/raw-records/#converting-xml-jobusagerecord","text":"The mapping from an XML JobUsageRecord to a JSON Raw GRACC record is outlined below; this should help inform how new records are generated as well. The raw XML record is stored in the RawXML field, to allow for later reference and remapping.","title":"Converting XML JobUsageRecord"},{"location":"dev-docs/raw-records/#identity-groups","text":"Identity groups are flattened by moving their sub-elements to the top level: RecordIdentity RecordId CreateTime JobIdentity GlobalJobId LocalJobId ProcessId (array) UserIdentity LocalUserId GlobalUsername CommonName DN VOName ReportableVOName","title":"Identity Groups"},{"location":"dev-docs/raw-records/#durations","text":"Duration fields are converted to seconds. CpuDuration can have usage \"user\" or \"system\", these are also moved into the top level: CpuDuration (combined) CpuDuration_user CpuDuration_system WallDuration","title":"Durations"},{"location":"dev-docs/raw-records/#resource","text":"Resources are transformed into a <description>:<value> map. The description is transformed to make it a valid field name (spaces and dots are converted to dashes) prefixed with Resource_ . Other properties are flattened as Resource_<description>_<property_name>:<property_value> . The special Resource ResourceType is moved directly to the top level. ResourceType BatchPilot is renamed Payload due to the former being misleading. TimeDuration and TimeInstant elements are likewise put in <type>:<value> maps with their respective prefixes. Durations are converted to seconds, discrete times are ISO8601 strings.","title":"Resource"},{"location":"dev-docs/raw-records/#other","text":"Any other elements are directly included in the top level. Properties of those elements are moved to fields named as <element>_<property> , e.g. JobName_description .","title":"Other"},{"location":"dev-docs/raw-records/#example-xml-record","text":"<JobUsageRecord xmlns=\"http://www.gridforum.org/2003/ur-wg\" xmlns:urwg=\"http://www.gridforum.org/2003/ur-wg\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gridforum.org/2003/ur-wg file:///u:/OSG/urwg-schema.11.xsd\"> <RecordIdentity urwg:createTime=\"2016-05-27T22:46:46Z\" urwg:recordId=\"osg-gw-7.t2.ucsd.edu:35741.2\"/> <JobIdentity> <GlobalJobId>condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242</GlobalJobId> <LocalJobId>185777</LocalJobId> </JobIdentity> <UserIdentity> <LocalUserId>cmsuser</LocalUserId> <GlobalUsername>cmsuser@t2.ucsd.edu</GlobalUsername> <DN>/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba</DN> <VOName>/cms/Role=production/Capability=NULL</VOName> <ReportableVOName>cms</ReportableVOName> </UserIdentity> <JobName>osg-gw-7.t2.ucsd.edu#185777.0#1464388242</JobName> <MachineName>osg-gw-7.t2.ucsd.edu</MachineName> <SubmitHost>osg-gw-7.t2.ucsd.edu</SubmitHost> <Status urwg:description=\"Condor Exit Status\">0</Status> <WallDuration urwg:description=\"Was entered in seconds\">PT10M17.0S</WallDuration> <TimeDuration urwg:type=\"RemoteUserCpu\">PT0S</TimeDuration> <TimeD <TimeDuration urwg:type=\"RemoteSysCpu\">PT18.0S</TimeDuration> <TimeDuration urwg:type=\"LocalSysCpu\">PT0S</TimeDuration> <TimeDuration urwg:type=\"CumulativeSuspensionTime\">PT0S</TimeDuration> <TimeDuration urwg:type=\"CommittedSuspensionTime\">PT0S</TimeDuration> <TimeDuration urwg:type=\"CommittedTime\">PT10M17.0S</TimeDuration> <CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"system\">PT18.0S</CpuDuration> <CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"user\">PT0S</CpuDuration> <EndTime urwg:description=\"Was entered in seconds\">2016-05-27T22:44:08Z</EndTime> <StartTime urwg:description=\"Was entered in seconds\">2016-05-27T22:33:51Z</StartTime> <Host primary=\"true\">cabinet-1-1-1.t2.ucsd.edu</Host> <Queue urwg:description=\"Condor's JobUniverse field\">5</Queue> <NodeCount urwg:metric=\"max\">1</NodeCount> <Processors urwg:metric=\"max\">1</Processors> <Resource urwg:description=\"CondorMyType\">Job</Resource> <Resource urwg:description=\"AccountingGroup\">group_cmsprod.cmsuser</Resource> <Resource urwg:description=\"ExitBySignal\">false</Resource> <Resource urwg:description=\"ExitCode\">0</Resource> <Resource urwg:description=\"condor.JobStatus\">4</Resource> <Network urwg:metric=\"total\" urwg:phaseUnit=\"PT10M17.0S\" urwg:storageUnit=\"b\">0</Network> <ProbeName>condor:osg-gw-7.t2.ucsd.edu</ProbeName> <SiteName>UCSDT2-D</SiteName> <Grid>OSG</Grid> <Njobs>1</Njobs> <Resource urwg:description=\"ResourceType\">Batch</Resource> </JobUsageRecord>","title":"Example XML Record"},{"location":"dev-docs/raw-records/#example-corresponding-json","text":"{ \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\", \"CreateTime\": \"2016-05-27T22:46:46Z\", \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\", \"LocalJobId\": \"185777\", \"LocalUserId\": \"cmsuser\", \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\", \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\", \"VOName\": \"/cms/Role=production/Capability=NULL\", \"ReportableVOName\": \"cms\", \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\", \"MachineName\": \"osg-gw-7.t2.ucsd.edu\", \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\", \"Status\": \"0\", \"Status_description\": \"Condor Exit Status\", \"WallDuration\": 617, \"WallDuration_description\": \"Was entered in seconds\" \"TimeDuration_CommittedSuspensionTime\": 0, \"TimeDuration_CommittedTime\": 617, \"TimeDuration_CumulativeSuspensionTime\": 0, \"TimeDuration_LocalSysCpu\": 0, \"TimeDuration_LocalUserCpu\": 0, \"TimeDuration_RemoteSysCpu\": 18, \"TimeDuration_RemoteUserCpu\": 0, \"CpuDuration\": 18, \"CpuDuration_system\": 18, \"CpuDuration_system_description\": \"Was entered in seconds\", \"CpuDuration_user\": 0, \"CpuDuration_user_description\": \"Was entered in seconds\", \"EndTime\": \"2016-05-27T22:44:08Z\", \"StartTime\": \"2016-05-27T22:33:51Z\", \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\", \"Queue\": \"5\", \"Queue_description\": \"Condor's JobUniverse field\", \"NodeCount\": \"1\", \"NodeCount_metric\": \"max\", \"Processors\": \"1\", \"Processors_metric\": \"max\", \"Resource_AccountingGroup\": \"group_cmsprod.cmsuser\", \"Resource_CondorMyType\": \"Job\", \"Resource_ExitBySignal\": \"false\", \"Resource_ExitCode\": \"0\", \"Resource_condor-JobStatus\": \"4\", \"ResourceType\": \"Batch\", \"Network\": \"0\", \"Network_metric\": \"total\", \"Network_phaseUnit\": 617, \"Network_storageUnit\": \"b\", \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\", \"SiteName\": \"UCSDT2-D\", \"Grid\": \"OSG\", \"Njobs\": \"1\", }","title":"Example Corresponding JSON"},{"location":"ops/services/","text":"GRACC Services These are the services the comprise the GRACC system, and details on where and how they are currently deployed on GRACE. Items marked with !! should be redesigned for production. Items marked with ?? should be evaluated for removal. (note: FIFE services will be moved to another Elasticsearch cluster \"soon\") External RabbitMQ Running at the GOC ITB: event-itb.grid.iu.edu:5672, :5671 (TLS), :15671 (HTTPS admin) Production: event.grid.iu.edu:5672, :5671 (TLS), :15671 (HTTPS admin) Head Node ( gratiav2-1 ) Openstack limits external access to ssh (:22) and http (:80 and :443). Elasticsearch (ingest node) Installed from official elastic repository Config in /etc/elasticsearch Running under systemd: elasticsearch.service Listening at: http://0.0.0.0:9200 and :9300 https://gracc.opensciencegrid.org/e/ ?? https://fifemon-es.fnal.gov External access requires certificate from GRACC private CA Certs and keys in /etc/nginx/certs Run make_cert COMMON_NAME to generate and sign a new certificate nginx allows very limited read-only access through public endpoints Elasticsearch (query node) !! Manual copy of ingest node ( s/elasticsearch/elasticsearch-ro/ ) Config in /etc/elasticsearch-ro Running under systemd: elasticsearch-ro.service Listening at: http://localhost:9201 https://gracc.opensciencegrid.org/q/ Includes readonlyrest plugin to allow limited unauthenticated read-only access from Kibana, Grafana, and to public queries. Configured in elasticsearch.yml https://readonlyrest.com GRACC Collectors and Stash agents GRACC Collectors ( gracc-collector ) accept records in formats provided by Gratia probes and collectors, transform them into GRACC JSON documents, and send them to the appropriate RabbitMQ exchange, which by convention is gracc.$STREAM.raw . Raw Stash agents ( gracc-stash-raw ) create a queue in RabbitMQ that is bound the the appropriate raw exchange, do minor manipulations to the documents (add checksum, calculate derived fields), and index the documents into elasticsearch ( gracc.$STREAM.raw$N-$YYYY.$MM ), where $N is a monotonic schema version number. !! A cron job, currently running under kretzke, generates aliases in Elasticsearch each month from gracc.$STREAM.raw$N-$YYYY.$MM to gracc.$STREAM.raw-$YYYY.$MM . Summary Stash agents ( gracc-stash-summary ) create a queue in RabbitMQ that is bound the the appropriate summary exchange, do minor manipulations to the documents (add checksum, calculate derived fields), and index the documents into elasticsearch ( gracc.$STREAM.summary$N-$YYYY ), where $N is a monotonic schema version number. !! Aliases are manually cretated in Elasticsearch from gracc.$STREAM.summary$N-$YYYY to gracc.$STREAM.summary (note just one alias for all years). Running under docker, via docker-compose Config in /etc/gracc/docker/$STREAM Within each directory is: docker-compose.yml - defines services for the stream .env - common environment variables for the stream's services Streams gracc.osg - \"main\" job stream Collector listening at: http://localhost:8180 http://gracc.opensciencegrid.org/gracc/osg gracc.osg-itb - ITB test stream Collector listening at: http://localhost:8181 http://gracc.opensciencegrid.org/gracc/osg-itb gracc.osg-transfer - transfers stream Collector listening at: http://localhost:8182 http://gracc.opensciencegrid.org/gracc/osg-transfer GRACC Request Agent The Request agent ( gracc-request ) listens on a RabbitMQ exchange ( gracc.$STREAM.requests ) for requests for \"replay\" or \"summarization\" of raw records for a given time period. The request includes a RabbitMQ exhchange the records should be sent to. !! Installed from copr (https://copr.fedorainfracloud.org/coprs/djw8605/GRACC/) Running under systemd: graccreq.service Config in /etc/graccreq/config.d/gracc-request.toml GRACC Summary Agent The Summary agent ( gracc-summary ) periodically requests summarizations, and has a CLI to manually summarize periods. !! Installed from copr (https://copr.fedorainfracloud.org/coprs/djw8605/GRACC/) Running under systemd: Every 15 minutes, resummarize the last 7 days: graccsumperiodic.service and graccsumperiodic.timer Every 24 hours, resummarize the last 365 days: graccsumperiodicyearly.service and graccsumperiodicyearly.timer Config in /etc/graccsum/config.d/gracc-summary.toml GRACC Reporting Images on dockerhub docker pull shreyb/gracc-reporting Currently running on cron jobs that run wrapper shell scripts that in turn call docker-compose. Thus, checking the status of report runs can be accomplished by using docker ps -a . Dockerfiles and docker-compose files can be pulled from Github to replicate the structure of the OSG deployment of the docker containers. Note that this does not contain report config files. See configuration details . Reports run from OSG ( osg-reports ): OSG Flocking Report (osgflockingreport) OSG Project Usage Report (osgreport) OSG Missing Projects from Records report (osgmissingprojects) OSG Usage Per Site Report (osgpersitereport) Top [N] Providers of Opportunistic Hours on the OSG (News Report) (osgtopoppusagereport) Gratia Probes that haven't reported in the past two days (osgprobereport) Configuring GRACC Reporting GRACC Reporting config file at /opt/gracc-reporting/config/osg.toml . Three main sections: General: General parameters such as admin emails, elasticsearch host, etc. Report-specific: Each top-level entry corresponds to a particular report (or a particular report) Databases: For any external database lookups, such as XSEDE. Not necessary unless you're running a report that uses external databases The source code repository on GitHub has sample config files that can be modified, installed wherever is desired, and then specified in the docker-compose files. Docker configuration is via docker-compose files at /opt/gracc-reporting/<report-subpackage>/<report-name>/docker-compose.yml . Here's an example (OSG Flocking Report). Note that we use environmental variables liberally, but that's not a requirement: version: \"2\" services: osgflockingreport: image: \"shreyb/gracc-reporting:osgflocking-report_${VERSIONRELEASE}\" volumes: - ${LOCALLOGDIR}:/tmp/log - ${CONFIGDIR}:/tmp/gracc-config - /etc/localtime:/etc/localtime network_mode: \"host\" command: [\"-s\", \"${starttime}\", \"-e\", \"${endtime}\", \"-c\", \"/tmp/gracc-config/osg.toml\", \"-L\", \"/tmp/log/osgflockingreport.log\"] APEL reporting Source at OSG docker docker {pull, run} opensciencegrid/gracc-apel Running under systemd: gracc-apel.service and gracc-apel.timer Registered service certificate with APEL admins needed in docker we use: $ openssl x509 -in /etc/grid-security/apel/apelcert.pem -noout -subject subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=apel/hcc-grace.unl.edu SELinux prevents inheritance of certificates, so this must be set on docker master node: chcon -Rt svirt_sandbox_file_t /etc/grid-security/apel/ Configuring GRACC-APEL in systemd gracc-apel.service configure, enable, start, test $ cat /lib/systemd/system/gracc-apel.service [Unit] Description=GRACC APEL reporting Docker container [Service] Type=oneshot ExecStart=/bin/docker run -v /etc/grid-security/apel/apelcert.pem:/etc/grid-security/apel/apelcert.pem -v /etc/grid-security/apel/apelkey.pem:/etc/grid-security/apel/apelkey.pem opensciencegrid/gracc-apel $ systemctl enabled gracc-apel.service $ systemctl start gracc-apel.service $ systemctl status gracc-apel.service To run periodically: * gracc-apel.timer configure, enable, start $ cat /lib/systemd/system/gracc-apel.timer [Unit] Description=Run GRACC-to-APEL reporting script [Timer] # Explicitly declare service that this timer is responsible for Unit=gracc-apel.service # Runs 'gracc-apel' relative to when the *timer-unit* has been activated OnActiveSec=1hour # Runs 'gracc-apel' relative to when *service-unit* was last deactivated OnUnitInactiveSec=1hour # Randomize runtime by a small amount each run. RandomizedDelaySec=2min [Install] WantedBy=timers.target $ systemctl enabled gracc-apel.timer $ systemctl start gracc-apel.timer $ systemctl status gracc-apel.timer To check if gracc-apel.timer runs: $ systemctl list-timers *apel* NEXT LEFT LAST PASSED UNIT ACTIVATES Mon 2017-05-22 17:48:37 CDT 21min left Mon 2017-05-22 16:46:40 CDT 40min ago gracc-apel.timer gracc-apel.service Grafana Installed from official Grafana yum repo Running under systemd: grafana-server.service Config in /etc/grafana Data in /var/lib/grafana Listening at: http://localhost:3000 https://gracc.opensciencegrid.org/ Kibana Installed from official Elasticsearch yum repo Running under systemd: kibana.service Config in /etc/kibana Listening at: http://localhost:5601 https://gracc.opensciencegrid.org/kibana/ readonlyrest Elasticsearch plugin controls access, write actions require authentication with basic auth, configured in /etc/elasticsearch-ro/elasticsearch.yml . Prometheus Prometheus is used for monitoring the nodes and services. !! Installed in /opt/prometheus Config in /etc/prometheus Data in /data/prometheus Running under systemd: prometheus.service Listening at: http://localhost:9090 https://gracc.opensciencegrid.org/prometheus !! external access requires basic auth, /etc/nginx/conf.d/kibana.htpasswd limited unauthenticated access to query endpoint Prometheus Exporters Prometheus exporters collect data from services and present them for collection by Prometheus. RabbitMQ Exporter !! manually run under docker (to be moved to docker-compose) configuration via env var RABBIT_URL=https://event-itb.grid.iu.edu:15671 RABBIT_USER=$USER RABBIT_PASSWORD=$PASSWORD Listening at http://localhost:9111 Node Exporter Installed at /opt/prometheus Run with systemd: prometheus-node-exporter.service Listening at http://0.0.0.0:9100 Graphite Exporter The Graphite exporter accepts time-series data in graphite format and exposes it for collection by prometheus. Running under docker Listening at: http://0.0.0.0:9100 (publish to prometheus) https://0.0.0.0:2003 (collect from graphite) Nginx All public services are proxied through nginx. Installed from EPEL Primary config in /etc/nginx/conf.d/default.conf Running under systemd: nginx.service Listening at: http://0.0.0.0:80 http://0.0.0.0:443 http://gracc.opensciencegrid.org https://gracc.opensciencegrid.org Docker Running under systemd: docker.service Extra config in /etc/systemd/system/docker.service.d Container logs sent to journald; e.g. sudo journalctl CONTAINER_NAME=graccosg_gracc-collector_1 Portainer Portainer is a web-based utility for monitoring and managing containers. running under docker listening on http://0.0.0.0:9000 !! not proxied through nginx, need to tunnel, e.g. ssh -L 9000:localhost:9000 gracc.opensciencegrid.org Data Nodes ( gratiav2-2 though -5 ) Ansible is set up on the head node to perform operations across all data nodes, e.g. sudo ansible elasticsearch -a 'uptime' . Configuration in /etc/ansible . Elasticsearch Installed from official elastic repository Config in /etc/elasticsearch Data in /data/elasticsearch and /data2/elasticsearch Listening at http://0.0.0.0:9200 and :9300 Prometheus Exporters Node Exporter Installed at /opt/prometheus Run with systemd: prometheus-elasticsearch-exporter.service Listening at http://0.0.0.0:9100 Elasticsearch Exporter Installed at /opt/prometheus Run with systemd: prometheus-node-exporter.service Listening at http://0.0.0.0:9108","title":"Operations"},{"location":"ops/services/#gracc-services","text":"These are the services the comprise the GRACC system, and details on where and how they are currently deployed on GRACE. Items marked with !! should be redesigned for production. Items marked with ?? should be evaluated for removal. (note: FIFE services will be moved to another Elasticsearch cluster \"soon\")","title":"GRACC Services"},{"location":"ops/services/#external","text":"","title":"External"},{"location":"ops/services/#rabbitmq","text":"Running at the GOC ITB: event-itb.grid.iu.edu:5672, :5671 (TLS), :15671 (HTTPS admin) Production: event.grid.iu.edu:5672, :5671 (TLS), :15671 (HTTPS admin)","title":"RabbitMQ"},{"location":"ops/services/#head-node-gratiav2-1","text":"Openstack limits external access to ssh (:22) and http (:80 and :443).","title":"Head Node (gratiav2-1)"},{"location":"ops/services/#elasticsearch-ingest-node","text":"Installed from official elastic repository Config in /etc/elasticsearch Running under systemd: elasticsearch.service Listening at: http://0.0.0.0:9200 and :9300 https://gracc.opensciencegrid.org/e/ ?? https://fifemon-es.fnal.gov External access requires certificate from GRACC private CA Certs and keys in /etc/nginx/certs Run make_cert COMMON_NAME to generate and sign a new certificate nginx allows very limited read-only access through public endpoints","title":"Elasticsearch (ingest node)"},{"location":"ops/services/#elasticsearch-query-node","text":"!! Manual copy of ingest node ( s/elasticsearch/elasticsearch-ro/ ) Config in /etc/elasticsearch-ro Running under systemd: elasticsearch-ro.service Listening at: http://localhost:9201 https://gracc.opensciencegrid.org/q/ Includes readonlyrest plugin to allow limited unauthenticated read-only access from Kibana, Grafana, and to public queries. Configured in elasticsearch.yml https://readonlyrest.com","title":"Elasticsearch (query node)"},{"location":"ops/services/#gracc-collectors-and-stash-agents","text":"GRACC Collectors ( gracc-collector ) accept records in formats provided by Gratia probes and collectors, transform them into GRACC JSON documents, and send them to the appropriate RabbitMQ exchange, which by convention is gracc.$STREAM.raw . Raw Stash agents ( gracc-stash-raw ) create a queue in RabbitMQ that is bound the the appropriate raw exchange, do minor manipulations to the documents (add checksum, calculate derived fields), and index the documents into elasticsearch ( gracc.$STREAM.raw$N-$YYYY.$MM ), where $N is a monotonic schema version number. !! A cron job, currently running under kretzke, generates aliases in Elasticsearch each month from gracc.$STREAM.raw$N-$YYYY.$MM to gracc.$STREAM.raw-$YYYY.$MM . Summary Stash agents ( gracc-stash-summary ) create a queue in RabbitMQ that is bound the the appropriate summary exchange, do minor manipulations to the documents (add checksum, calculate derived fields), and index the documents into elasticsearch ( gracc.$STREAM.summary$N-$YYYY ), where $N is a monotonic schema version number. !! Aliases are manually cretated in Elasticsearch from gracc.$STREAM.summary$N-$YYYY to gracc.$STREAM.summary (note just one alias for all years). Running under docker, via docker-compose Config in /etc/gracc/docker/$STREAM Within each directory is: docker-compose.yml - defines services for the stream .env - common environment variables for the stream's services","title":"GRACC Collectors and Stash agents"},{"location":"ops/services/#streams","text":"gracc.osg - \"main\" job stream Collector listening at: http://localhost:8180 http://gracc.opensciencegrid.org/gracc/osg gracc.osg-itb - ITB test stream Collector listening at: http://localhost:8181 http://gracc.opensciencegrid.org/gracc/osg-itb gracc.osg-transfer - transfers stream Collector listening at: http://localhost:8182 http://gracc.opensciencegrid.org/gracc/osg-transfer","title":"Streams"},{"location":"ops/services/#gracc-request-agent","text":"The Request agent ( gracc-request ) listens on a RabbitMQ exchange ( gracc.$STREAM.requests ) for requests for \"replay\" or \"summarization\" of raw records for a given time period. The request includes a RabbitMQ exhchange the records should be sent to. !! Installed from copr (https://copr.fedorainfracloud.org/coprs/djw8605/GRACC/) Running under systemd: graccreq.service Config in /etc/graccreq/config.d/gracc-request.toml","title":"GRACC Request Agent"},{"location":"ops/services/#gracc-summary-agent","text":"The Summary agent ( gracc-summary ) periodically requests summarizations, and has a CLI to manually summarize periods. !! Installed from copr (https://copr.fedorainfracloud.org/coprs/djw8605/GRACC/) Running under systemd: Every 15 minutes, resummarize the last 7 days: graccsumperiodic.service and graccsumperiodic.timer Every 24 hours, resummarize the last 365 days: graccsumperiodicyearly.service and graccsumperiodicyearly.timer Config in /etc/graccsum/config.d/gracc-summary.toml","title":"GRACC Summary Agent"},{"location":"ops/services/#gracc-reporting","text":"Images on dockerhub docker pull shreyb/gracc-reporting Currently running on cron jobs that run wrapper shell scripts that in turn call docker-compose. Thus, checking the status of report runs can be accomplished by using docker ps -a . Dockerfiles and docker-compose files can be pulled from Github to replicate the structure of the OSG deployment of the docker containers. Note that this does not contain report config files. See configuration details . Reports run from OSG ( osg-reports ): OSG Flocking Report (osgflockingreport) OSG Project Usage Report (osgreport) OSG Missing Projects from Records report (osgmissingprojects) OSG Usage Per Site Report (osgpersitereport) Top [N] Providers of Opportunistic Hours on the OSG (News Report) (osgtopoppusagereport) Gratia Probes that haven't reported in the past two days (osgprobereport)","title":"GRACC Reporting"},{"location":"ops/services/#configuring-gracc-reporting","text":"GRACC Reporting config file at /opt/gracc-reporting/config/osg.toml . Three main sections: General: General parameters such as admin emails, elasticsearch host, etc. Report-specific: Each top-level entry corresponds to a particular report (or a particular report) Databases: For any external database lookups, such as XSEDE. Not necessary unless you're running a report that uses external databases The source code repository on GitHub has sample config files that can be modified, installed wherever is desired, and then specified in the docker-compose files. Docker configuration is via docker-compose files at /opt/gracc-reporting/<report-subpackage>/<report-name>/docker-compose.yml . Here's an example (OSG Flocking Report). Note that we use environmental variables liberally, but that's not a requirement: version: \"2\" services: osgflockingreport: image: \"shreyb/gracc-reporting:osgflocking-report_${VERSIONRELEASE}\" volumes: - ${LOCALLOGDIR}:/tmp/log - ${CONFIGDIR}:/tmp/gracc-config - /etc/localtime:/etc/localtime network_mode: \"host\" command: [\"-s\", \"${starttime}\", \"-e\", \"${endtime}\", \"-c\", \"/tmp/gracc-config/osg.toml\", \"-L\", \"/tmp/log/osgflockingreport.log\"]","title":"Configuring GRACC Reporting"},{"location":"ops/services/#apel-reporting","text":"Source at OSG docker docker {pull, run} opensciencegrid/gracc-apel Running under systemd: gracc-apel.service and gracc-apel.timer Registered service certificate with APEL admins needed in docker we use: $ openssl x509 -in /etc/grid-security/apel/apelcert.pem -noout -subject subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=apel/hcc-grace.unl.edu SELinux prevents inheritance of certificates, so this must be set on docker master node: chcon -Rt svirt_sandbox_file_t /etc/grid-security/apel/","title":"APEL reporting"},{"location":"ops/services/#configuring-gracc-apel-in-systemd","text":"gracc-apel.service configure, enable, start, test $ cat /lib/systemd/system/gracc-apel.service [Unit] Description=GRACC APEL reporting Docker container [Service] Type=oneshot ExecStart=/bin/docker run -v /etc/grid-security/apel/apelcert.pem:/etc/grid-security/apel/apelcert.pem -v /etc/grid-security/apel/apelkey.pem:/etc/grid-security/apel/apelkey.pem opensciencegrid/gracc-apel $ systemctl enabled gracc-apel.service $ systemctl start gracc-apel.service $ systemctl status gracc-apel.service To run periodically: * gracc-apel.timer configure, enable, start $ cat /lib/systemd/system/gracc-apel.timer [Unit] Description=Run GRACC-to-APEL reporting script [Timer] # Explicitly declare service that this timer is responsible for Unit=gracc-apel.service # Runs 'gracc-apel' relative to when the *timer-unit* has been activated OnActiveSec=1hour # Runs 'gracc-apel' relative to when *service-unit* was last deactivated OnUnitInactiveSec=1hour # Randomize runtime by a small amount each run. RandomizedDelaySec=2min [Install] WantedBy=timers.target $ systemctl enabled gracc-apel.timer $ systemctl start gracc-apel.timer $ systemctl status gracc-apel.timer To check if gracc-apel.timer runs: $ systemctl list-timers *apel* NEXT LEFT LAST PASSED UNIT ACTIVATES Mon 2017-05-22 17:48:37 CDT 21min left Mon 2017-05-22 16:46:40 CDT 40min ago gracc-apel.timer gracc-apel.service","title":"Configuring GRACC-APEL in systemd"},{"location":"ops/services/#grafana","text":"Installed from official Grafana yum repo Running under systemd: grafana-server.service Config in /etc/grafana Data in /var/lib/grafana Listening at: http://localhost:3000 https://gracc.opensciencegrid.org/","title":"Grafana"},{"location":"ops/services/#kibana","text":"Installed from official Elasticsearch yum repo Running under systemd: kibana.service Config in /etc/kibana Listening at: http://localhost:5601 https://gracc.opensciencegrid.org/kibana/ readonlyrest Elasticsearch plugin controls access, write actions require authentication with basic auth, configured in /etc/elasticsearch-ro/elasticsearch.yml .","title":"Kibana"},{"location":"ops/services/#prometheus","text":"Prometheus is used for monitoring the nodes and services. !! Installed in /opt/prometheus Config in /etc/prometheus Data in /data/prometheus Running under systemd: prometheus.service Listening at: http://localhost:9090 https://gracc.opensciencegrid.org/prometheus !! external access requires basic auth, /etc/nginx/conf.d/kibana.htpasswd limited unauthenticated access to query endpoint","title":"Prometheus"},{"location":"ops/services/#prometheus-exporters","text":"Prometheus exporters collect data from services and present them for collection by Prometheus.","title":"Prometheus Exporters"},{"location":"ops/services/#rabbitmq-exporter","text":"!! manually run under docker (to be moved to docker-compose) configuration via env var RABBIT_URL=https://event-itb.grid.iu.edu:15671 RABBIT_USER=$USER RABBIT_PASSWORD=$PASSWORD Listening at http://localhost:9111","title":"RabbitMQ Exporter"},{"location":"ops/services/#node-exporter","text":"Installed at /opt/prometheus Run with systemd: prometheus-node-exporter.service Listening at http://0.0.0.0:9100","title":"Node Exporter"},{"location":"ops/services/#graphite-exporter","text":"The Graphite exporter accepts time-series data in graphite format and exposes it for collection by prometheus. Running under docker Listening at: http://0.0.0.0:9100 (publish to prometheus) https://0.0.0.0:2003 (collect from graphite)","title":"Graphite Exporter"},{"location":"ops/services/#nginx","text":"All public services are proxied through nginx. Installed from EPEL Primary config in /etc/nginx/conf.d/default.conf Running under systemd: nginx.service Listening at: http://0.0.0.0:80 http://0.0.0.0:443 http://gracc.opensciencegrid.org https://gracc.opensciencegrid.org","title":"Nginx"},{"location":"ops/services/#docker","text":"Running under systemd: docker.service Extra config in /etc/systemd/system/docker.service.d Container logs sent to journald; e.g. sudo journalctl CONTAINER_NAME=graccosg_gracc-collector_1","title":"Docker"},{"location":"ops/services/#portainer","text":"Portainer is a web-based utility for monitoring and managing containers. running under docker listening on http://0.0.0.0:9000 !! not proxied through nginx, need to tunnel, e.g. ssh -L 9000:localhost:9000 gracc.opensciencegrid.org","title":"Portainer"},{"location":"ops/services/#data-nodes-gratiav2-2-though-5","text":"Ansible is set up on the head node to perform operations across all data nodes, e.g. sudo ansible elasticsearch -a 'uptime' . Configuration in /etc/ansible .","title":"Data Nodes (gratiav2-2 though -5)"},{"location":"ops/services/#elasticsearch","text":"Installed from official elastic repository Config in /etc/elasticsearch Data in /data/elasticsearch and /data2/elasticsearch Listening at http://0.0.0.0:9200 and :9300","title":"Elasticsearch"},{"location":"ops/services/#prometheus-exporters_1","text":"","title":"Prometheus Exporters"},{"location":"ops/services/#node-exporter_1","text":"Installed at /opt/prometheus Run with systemd: prometheus-elasticsearch-exporter.service Listening at http://0.0.0.0:9100","title":"Node Exporter"},{"location":"ops/services/#elasticsearch-exporter","text":"Installed at /opt/prometheus Run with systemd: prometheus-node-exporter.service Listening at http://0.0.0.0:9108","title":"Elasticsearch Exporter"},{"location":"ops/troubleshoot/","text":"Troubleshooting of GRACC services Helpful dashboards GRACC Service Status GRACC Collector Stats RabbitMQ queues Probe Record Rate - example for given CE in addition, check on Kibana ProbeName records OSG Connect Summary - UChicago Site Transfer Summary Institutions contributing to the OSG by name Issues Selection of issues being investigated and actions taken in order to resolve them. Logstash Symptom : high usage of gracc archiver memory (e.g. ~12GB) logstash seems to be backed up and not responding RabbitMQ has high volume of queued messages (e.g. ~100k) Action : systemctl restart elasticsearch.service added to check_mk systemd monitoring for elasticsearch and elasticsearch-ro for continuous high rate disconnections in RabbitMQ contact Marina Krenz GRACC-APEL Update missing records It may happen site has problem with sending accouting data to GRACC in particular month so when fixed they ask us correct accoutning in APEL report. In such case do: 1) From hcc-grace-itb.unl.edu run manually $ cd /root/gracc-apel/; ./apel_report YYYY MM 2) Move file $ mv /root/gracc-apel/MM_YYYY.apel /var/spool/apel/outgoing/12345678/1234567890abcd 3) Send off $ ssmsend","title":"Troubleshooting"},{"location":"ops/troubleshoot/#troubleshooting-of-gracc-services","text":"","title":"Troubleshooting of GRACC services"},{"location":"ops/troubleshoot/#helpful-dashboards","text":"GRACC Service Status GRACC Collector Stats RabbitMQ queues Probe Record Rate - example for given CE in addition, check on Kibana ProbeName records OSG Connect Summary - UChicago Site Transfer Summary Institutions contributing to the OSG by name","title":"Helpful dashboards"},{"location":"ops/troubleshoot/#issues","text":"Selection of issues being investigated and actions taken in order to resolve them.","title":"Issues"},{"location":"ops/troubleshoot/#logstash","text":"Symptom : high usage of gracc archiver memory (e.g. ~12GB) logstash seems to be backed up and not responding RabbitMQ has high volume of queued messages (e.g. ~100k) Action : systemctl restart elasticsearch.service added to check_mk systemd monitoring for elasticsearch and elasticsearch-ro for continuous high rate disconnections in RabbitMQ contact Marina Krenz","title":"Logstash"},{"location":"ops/troubleshoot/#gracc-apel","text":"","title":"GRACC-APEL"},{"location":"ops/troubleshoot/#update-missing-records","text":"It may happen site has problem with sending accouting data to GRACC in particular month so when fixed they ask us correct accoutning in APEL report. In such case do: 1) From hcc-grace-itb.unl.edu run manually $ cd /root/gracc-apel/; ./apel_report YYYY MM 2) Move file $ mv /root/gracc-apel/MM_YYYY.apel /var/spool/apel/outgoing/12345678/1234567890abcd 3) Send off $ ssmsend","title":"Update missing records"},{"location":"user/direct/","text":"Direct Access GRACC runs on Elasticsearch , which can be accessed via many client libraries and tools. A read-only endpoint into the GRACC Elasticsearch is available at https://gracc.opensciencegrid.org/q. cURL The Elasticsearch query DSL is quite complex, but is also very powerful. Below is a relatively simple but non-trivial example of directly querying GRACC from the command-line via cURL. This query will calculate the number of payload jobs run by LIGO in January 2017, and the total wall time they used. curl --header 'Content-Type: application/json' 'https://gracc.opensciencegrid.org/q/gracc.osg.summary/_search?pretty' --data-binary ' { \"query\": { \"query_string\": { \"query\": \"VOName:ligo AND ResourceType:Payload AND EndTime:[2017-01-01 TO 2017-02-01]\" } }, \"aggs\": { \"walltime\": { \"sum\": { \"field\": \"CoreHours\" } }, \"jobs\": { \"sum\": { \"field\": \"Njobs\" } } }, \"size\": 0 }' Which might return: { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 }, \"hits\" : { \"total\" : 68, \"max_score\" : 0.0, \"hits\" : [ ] }, \"aggregations\" : { \"jobs\" : { \"value\" : 20626.0 }, \"walltime\" : { \"value\" : 41323.43916666666 } } } Compare to the VO Summary dashboard in Grafana. Python (elasticsearch-dsl) In this example we use the elasticsearch-dsl package to query the cumulative walltime and cpu time for some selected VOs for the past 90 day. We'll then calculate the CPU efficiency for each. We'll then load the results into a pandas dataframe for further analysis and plotting with matplotlib. Query Elasticsearch from elasticsearch import Elasticsearch gracc=Elasticsearch('https://gracc.opensciencegrid.org/q',timeout=120) from elasticsearch_dsl import Search # All FIFE VOs running on GPGrid q='ProbeName:*fifebatch* AND Host_description:GPGrid' # Last 90 days start='now-90d' end='now' # GRACC summary index i='gracc.osg.summary' s = Search(using=gracc, index=i) \\ .filter(\"range\",EndTime={'gte':start,'lte':end}) \\ .query(\"query_string\", query=q) s.aggs.bucket('vo', 'terms', field='VOName', size=20, order={'WallHours':'desc'}) \\ .metric('WallHours', 'sum', field='CoreHours') \\ .metric('UserCpuSeconds', 'sum', field='CpuDuration_user') \\ .metric('SysCpuSeconds', 'sum', field='CpuDuration_system') #print s.to_dict() r = s.execute() Print buckets for vo in r.aggregations.vo.buckets: eff = (vo.UserCpuSeconds.value+vo.SysCpuSeconds.value)/vo.WallHours.value/3600.0*100.0 print '%20s\\t%10.0f\\t%5.2f%%' % (vo.key, vo.WallHours.value, eff) nova 6144687 61.32% minos 4332035 92.74% mu2e 4143032 87.56% microboone 3085974 68.13% minerva 2600511 62.41% mars 2030271 84.34% dune 1576534 82.17% des 1523825 40.31% seaquest 944561 69.49% cdf 722131 72.49% gm2 621704 75.38% darkside 533901 41.15% lariat 256586 46.81% annie 137215 53.45% cdms 88995 85.85% sbnd 84224 76.48% argoneut 9975 85.41% fermilab 8294 95.52% genie 3386 72.48% coupp 2834 20.46% Pull aggregation results into pandas from pandas.io.json import json_normalize #df = json_normalize(r.aggs.vo.buckets.__dict__['_l_']) df = json_normalize(r.to_dict()['aggregations']['vo']['buckets']) df.head() SysCpuSeconds.value UserCpuSeconds.value WallHours.value doc_count key 0 51333597.0 1.350929e+10 6.140375e+06 2868 nova 1 30771554.0 1.435938e+10 4.311367e+06 746 minos 2 111866383.0 1.294148e+10 4.140957e+06 1002 mu2e 3 313454481.0 7.251906e+09 3.080024e+06 3355 microboone 4 37939287.0 5.800780e+09 2.599362e+06 1734 minerva %matplotlib inline df.plot.barh(x='key',y='WallHours.value') Calculate efficiency def efficiency(row): return (row['UserCpuSeconds.value']+row['SysCpuSeconds.value'])/row['WallHours.value']/3600.0 df['efficiency']=df.apply(efficiency,axis=1) df.plot.barh(x='key',y='efficiency',legend=False)","title":"Direct Queries"},{"location":"user/direct/#direct-access","text":"GRACC runs on Elasticsearch , which can be accessed via many client libraries and tools. A read-only endpoint into the GRACC Elasticsearch is available at https://gracc.opensciencegrid.org/q.","title":"Direct Access"},{"location":"user/direct/#curl","text":"The Elasticsearch query DSL is quite complex, but is also very powerful. Below is a relatively simple but non-trivial example of directly querying GRACC from the command-line via cURL. This query will calculate the number of payload jobs run by LIGO in January 2017, and the total wall time they used. curl --header 'Content-Type: application/json' 'https://gracc.opensciencegrid.org/q/gracc.osg.summary/_search?pretty' --data-binary ' { \"query\": { \"query_string\": { \"query\": \"VOName:ligo AND ResourceType:Payload AND EndTime:[2017-01-01 TO 2017-02-01]\" } }, \"aggs\": { \"walltime\": { \"sum\": { \"field\": \"CoreHours\" } }, \"jobs\": { \"sum\": { \"field\": \"Njobs\" } } }, \"size\": 0 }' Which might return: { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 }, \"hits\" : { \"total\" : 68, \"max_score\" : 0.0, \"hits\" : [ ] }, \"aggregations\" : { \"jobs\" : { \"value\" : 20626.0 }, \"walltime\" : { \"value\" : 41323.43916666666 } } } Compare to the VO Summary dashboard in Grafana.","title":"cURL"},{"location":"user/direct/#python-elasticsearch-dsl","text":"In this example we use the elasticsearch-dsl package to query the cumulative walltime and cpu time for some selected VOs for the past 90 day. We'll then calculate the CPU efficiency for each. We'll then load the results into a pandas dataframe for further analysis and plotting with matplotlib.","title":"Python (elasticsearch-dsl)"},{"location":"user/direct/#query-elasticsearch","text":"from elasticsearch import Elasticsearch gracc=Elasticsearch('https://gracc.opensciencegrid.org/q',timeout=120) from elasticsearch_dsl import Search # All FIFE VOs running on GPGrid q='ProbeName:*fifebatch* AND Host_description:GPGrid' # Last 90 days start='now-90d' end='now' # GRACC summary index i='gracc.osg.summary' s = Search(using=gracc, index=i) \\ .filter(\"range\",EndTime={'gte':start,'lte':end}) \\ .query(\"query_string\", query=q) s.aggs.bucket('vo', 'terms', field='VOName', size=20, order={'WallHours':'desc'}) \\ .metric('WallHours', 'sum', field='CoreHours') \\ .metric('UserCpuSeconds', 'sum', field='CpuDuration_user') \\ .metric('SysCpuSeconds', 'sum', field='CpuDuration_system') #print s.to_dict() r = s.execute()","title":"Query Elasticsearch"},{"location":"user/direct/#print-buckets","text":"for vo in r.aggregations.vo.buckets: eff = (vo.UserCpuSeconds.value+vo.SysCpuSeconds.value)/vo.WallHours.value/3600.0*100.0 print '%20s\\t%10.0f\\t%5.2f%%' % (vo.key, vo.WallHours.value, eff) nova 6144687 61.32% minos 4332035 92.74% mu2e 4143032 87.56% microboone 3085974 68.13% minerva 2600511 62.41% mars 2030271 84.34% dune 1576534 82.17% des 1523825 40.31% seaquest 944561 69.49% cdf 722131 72.49% gm2 621704 75.38% darkside 533901 41.15% lariat 256586 46.81% annie 137215 53.45% cdms 88995 85.85% sbnd 84224 76.48% argoneut 9975 85.41% fermilab 8294 95.52% genie 3386 72.48% coupp 2834 20.46%","title":"Print buckets"},{"location":"user/direct/#pull-aggregation-results-into-pandas","text":"from pandas.io.json import json_normalize #df = json_normalize(r.aggs.vo.buckets.__dict__['_l_']) df = json_normalize(r.to_dict()['aggregations']['vo']['buckets']) df.head() SysCpuSeconds.value UserCpuSeconds.value WallHours.value doc_count key 0 51333597.0 1.350929e+10 6.140375e+06 2868 nova 1 30771554.0 1.435938e+10 4.311367e+06 746 minos 2 111866383.0 1.294148e+10 4.140957e+06 1002 mu2e 3 313454481.0 7.251906e+09 3.080024e+06 3355 microboone 4 37939287.0 5.800780e+09 2.599362e+06 1734 minerva %matplotlib inline df.plot.barh(x='key',y='WallHours.value')","title":"Pull aggregation results into pandas"},{"location":"user/direct/#calculate-efficiency","text":"def efficiency(row): return (row['UserCpuSeconds.value']+row['SysCpuSeconds.value'])/row['WallHours.value']/3600.0 df['efficiency']=df.apply(efficiency,axis=1) df.plot.barh(x='key',y='efficiency',legend=False)","title":"Calculate efficiency"},{"location":"user/grafana/","text":"Accessing GRACC from Grafana Existing Grafana If you already have a Grafana instance that you maintain, it's very easy to add a GRACC datasource so you can start incorporating accounting data into your dashboards. Datasource Settings Setting Values Type Elasticsearch Url https://gracc.opensciencegrid.org/q Access Proxy (either should work) Index name gracc.osg.summary Pattern no pattern Time field name @timestamp Version 5.x Group by time interval >1d","title":"Custom Grafana"},{"location":"user/grafana/#accessing-gracc-from-grafana","text":"","title":"Accessing GRACC from Grafana"},{"location":"user/grafana/#existing-grafana","text":"If you already have a Grafana instance that you maintain, it's very easy to add a GRACC datasource so you can start incorporating accounting data into your dashboards.","title":"Existing Grafana"},{"location":"user/grafana/#datasource-settings","text":"Setting Values Type Elasticsearch Url https://gracc.opensciencegrid.org/q Access Proxy (either should work) Index name gracc.osg.summary Pattern no pattern Time field name @timestamp Version 5.x Group by time interval >1d","title":"Datasource Settings"},{"location":"user/standard/","text":"Standard GRACC Interfaces Grafana Grafana is an open-source graphing/dashboarding web application, widely used for system and service monitoring. The GRACC project maintains a Grafana deployment with a collection of standard dashboards at https://gracc.opensciencegrid.org . Kibana While Grafana is great for looking at pre-defined views into the data, it's not great for exploration or ad-hoc analytics, which is the area Kibana excels in. The GRACC project maintains a public read-only Kibana interface at https://gracc.opensciencegrid.org/kibana .","title":"Standard Interfaces"},{"location":"user/standard/#standard-gracc-interfaces","text":"","title":"Standard GRACC Interfaces"},{"location":"user/standard/#grafana","text":"Grafana is an open-source graphing/dashboarding web application, widely used for system and service monitoring. The GRACC project maintains a Grafana deployment with a collection of standard dashboards at https://gracc.opensciencegrid.org .","title":"Grafana"},{"location":"user/standard/#kibana","text":"While Grafana is great for looking at pre-defined views into the data, it's not great for exploration or ad-hoc analytics, which is the area Kibana excels in. The GRACC project maintains a public read-only Kibana interface at https://gracc.opensciencegrid.org/kibana .","title":"Kibana"}]}